{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe merges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHANGE DATES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.today()\n",
    "yesterday = datetime.today() - timedelta(days=1)\n",
    "yesterday_str = yesterday.strftime(\"%d%m%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changedate = today.strftime(\"%d%m%Y\")\n",
    "changedate1 = today.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_path = rf'C:\\Users\\muhammadadlan\\Downloads\\customer_ultimate_beta\\transformed_data'\n",
    "\n",
    "directory_path = rf'{transformed_path}\\{changedate}'\n",
    "\n",
    "os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "print(f\"Directory '{directory_path}' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = rf'C:\\Users\\muhammadadlan\\Downloads\\customer_ultimate_beta\\raw_data'\n",
    "path2 = rf'C:\\Users\\muhammadadlan\\Downloads\\customer_ultimate_beta'\n",
    "path3 = rf'C:\\Users\\muhammadadlan\\Downloads\\customer_ultimate_beta\\transformed_data\\{yesterday_str}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer1 = pd.read_csv(fr'{path3}\\customer_v2.csv', low_memory=False, dtype={'id':str, 'nric':str})\n",
    "customer2 = pd.read_csv(fr'{path2}\\bronze_customer\\{changedate}\\data_{changedate1}.csv', dtype={'id':str, 'nric':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is called 'customer_data'\n",
    "columns_to_drop = [col for col in customer2.columns if 'devices_' in col and int(col.split('_')[1]) > 0]\n",
    "\n",
    "customer2 = customer2.drop(columns=columns_to_drop)\n",
    "\n",
    "# Sort the dataframe\n",
    "customer2 = customer2.sort_values(['id', 'version', 'lastUpdate'], ascending=[True, True, True])\n",
    "\n",
    "# Drop duplicates based on the 'id' column, keeping the last occurrence\n",
    "customer2 = customer2.drop_duplicates(subset=['id'], keep='last')\n",
    "\n",
    "customer2 = customer2.loc[:, ~customer2.columns.str.contains('^Unnamed')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_customer = pd.concat([customer1, customer2], ignore_index=True)\n",
    "\n",
    "duplicated_rows = concatenated_customer[concatenated_customer.duplicated()]\n",
    "\n",
    "print(\"Duplicated rows to be dropped:\")\n",
    "print(duplicated_rows)\n",
    "\n",
    "concatenated_customer = concatenated_customer.drop_duplicates()\n",
    "\n",
    "concatenated_customer = concatenated_customer.loc[:, ~concatenated_customer.columns.str.contains('^Unnamed')]\n",
    "\n",
    "concatenated_customer.to_csv(rf'transformed_data\\{changedate}\\customer_test_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Savings Acc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa1 = pd.read_csv(fr'{path1}\\sa_full.csv', dtype={'customerId':str})\n",
    "sa2 = pd.read_csv(fr'{path2}\\bronze_saving_acc\\{changedate}\\data_{changedate1}.csv', dtype={'customerId':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_sa = pd.concat([sa1, sa2], ignore_index=True)\n",
    "\n",
    "duplicated_rows = concatenated_sa[concatenated_sa.duplicated()]\n",
    "\n",
    "print(\"Duplicated rows to be dropped:\")\n",
    "print(duplicated_rows)\n",
    "\n",
    "concatenated_sa = concatenated_sa.drop_duplicates()\n",
    "\n",
    "concatenated_sa = concatenated_sa.loc[:, ~concatenated_sa.columns.str.contains('^Unnamed')]\n",
    "\n",
    "concatenated_sa.to_csv(rf'raw_data\\sa_full.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Savings Pot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp1 = pd.read_csv(fr'{path1}\\sp_full.csv', dtype={'customerId':str})\n",
    "sp2 = pd.read_csv(fr'{path2}\\bronze_savings_pot\\{changedate}\\data_{changedate1}.csv', dtype={'customerId':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_sp = pd.concat([sp1, sp2], ignore_index=True)\n",
    "\n",
    "duplicated_rows = concatenated_sp[concatenated_sp.duplicated()]\n",
    "\n",
    "print(\"Duplicated rows to be dropped:\")\n",
    "print(duplicated_rows)\n",
    "\n",
    "concatenated_sp = concatenated_sp.drop_duplicates()\n",
    "\n",
    "concatenated_sp = concatenated_sp.loc[:, ~concatenated_sp.columns.str.contains('^Unnamed')]\n",
    "\n",
    "concatenated_sp.to_csv(rf'raw_data\\sp_full.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Payments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payments1 = pd.read_csv(fr'{path1}\\payments_full.csv', dtype={'customerId':str})\n",
    "payments2 = pd.read_csv(fr'{path2}\\bronze_payments\\{changedate}\\data_{changedate1}.csv', dtype={'customerId':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_payments = pd.concat([payments1, payments2], ignore_index=True)\n",
    "\n",
    "duplicated_rows = concatenated_payments[concatenated_payments.duplicated()]\n",
    "\n",
    "print(\"Duplicated rows to be dropped:\")\n",
    "print(duplicated_rows)\n",
    "\n",
    "concatenated_payments = concatenated_payments.drop_duplicates()\n",
    "\n",
    "concatenated_payments = concatenated_payments.loc[:, ~concatenated_payments.columns.str.contains('^Unnamed')]\n",
    "\n",
    "concatenated_payments.to_csv(rf'raw_data\\payments_full.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FTV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftv1 = pd.read_csv(fr'{path1}\\ftv_full.csv', dtype={'customerId':str})\n",
    "ftv2 = pd.read_csv(fr'{path2}\\bronze_ftv\\{changedate}\\data_{changedate1}.csv', dtype={'customerId':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_ftv = pd.concat([ftv1, ftv2], ignore_index=True)\n",
    "\n",
    "duplicated_rows = concatenated_ftv[concatenated_ftv.duplicated()]\n",
    "\n",
    "print(\"Duplicated rows to be dropped:\")\n",
    "print(duplicated_rows)\n",
    "\n",
    "concatenated_ftv = concatenated_ftv.drop_duplicates()\n",
    "\n",
    "concatenated_ftv = concatenated_ftv.loc[:, ~concatenated_ftv.columns.str.contains('^Unnamed')]\n",
    "\n",
    "concatenated_ftv.to_csv(rf'raw_data\\ftv_full.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reversal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reversal1 = pd.read_csv(fr'{path1}\\reversal_full.csv', dtype={'customerId':str})\n",
    "reversal2 = pd.read_csv(fr'{path2}\\bronze_reversal\\{changedate}\\data_{changedate1}.csv', dtype={'customerId':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_reversal = pd.concat([reversal1, reversal2], ignore_index=True)\n",
    "\n",
    "duplicated_rows = concatenated_reversal[concatenated_reversal.duplicated()]\n",
    "\n",
    "print(\"Duplicated rows to be dropped:\")\n",
    "print(duplicated_rows)\n",
    "\n",
    "concatenated_reversal = concatenated_reversal.drop_duplicates()\n",
    "\n",
    "concatenated_reversal = concatenated_reversal.loc[:, ~concatenated_reversal.columns.str.contains('^Unnamed')]\n",
    "\n",
    "concatenated_reversal.to_csv(rf'raw_data\\reversal_full.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Address:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address1 = pd.read_csv(fr'{path1}\\address_full.csv')\n",
    "address2 = pd.read_csv(fr'{path2}\\bronze_address\\{changedate}\\data_{changedate1}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_address = pd.concat([address1, address2], ignore_index=True)\n",
    "\n",
    "duplicated_rows = concatenated_address[concatenated_address.duplicated()]\n",
    "\n",
    "print(\"Duplicated rows to be dropped:\")\n",
    "print(duplicated_rows)\n",
    "\n",
    "concatenated_address = concatenated_address.drop_duplicates()\n",
    "\n",
    "concatenated_address = concatenated_address.loc[:, ~concatenated_address.columns.str.contains('^Unnamed')]\n",
    "\n",
    "concatenated_address.to_csv(rf'raw_data\\address_full.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Employment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employment1 = pd.read_csv(fr'{path1}\\employment_full.csv')\n",
    "employment2 = pd.read_csv(fr'{path2}\\bronze_employment\\{changedate}\\data_{changedate1}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_employment = pd.concat([employment1, employment2], ignore_index=True)\n",
    "\n",
    "duplicated_rows = concatenated_employment[concatenated_employment.duplicated()]\n",
    "\n",
    "print(\"Duplicated rows to be dropped:\")\n",
    "print(duplicated_rows)\n",
    "\n",
    "concatenated_employment = concatenated_employment.drop_duplicates()\n",
    "\n",
    "concatenated_employment = concatenated_employment.loc[:, ~concatenated_employment.columns.str.contains('^Unnamed')]\n",
    "\n",
    "concatenated_employment.to_csv(rf'raw_data\\employment_full.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile1 = pd.read_csv(fr'{path1}\\profile_full.csv')\n",
    "profile2 = pd.read_csv(fr'{path2}\\bronze_profile\\{changedate}\\data_{changedate1}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_profile = pd.concat([profile1, profile2], ignore_index=True)\n",
    "\n",
    "duplicated_rows = concatenated_profile[concatenated_profile.duplicated()]\n",
    "\n",
    "print(\"Duplicated rows to be dropped:\")\n",
    "print(duplicated_rows)\n",
    "\n",
    "concatenated_profile = concatenated_profile.drop_duplicates()\n",
    "\n",
    "concatenated_profile = concatenated_profile.loc[:, ~concatenated_profile.columns.str.contains('^Unnamed')]\n",
    "\n",
    "concatenated_profile.to_csv(fr'raw_data\\profile_full.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crr1 = pd.read_csv(fr'{path1}\\crr_full.csv')\n",
    "crr2 = pd.read_csv(fr'{path2}\\bronze_crr\\{changedate}\\data_{changedate1}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_crr = pd.concat([crr1, crr2], ignore_index=True)\n",
    "\n",
    "duplicated_rows = concatenated_crr[concatenated_crr.duplicated()]\n",
    "\n",
    "print(\"Duplicated rows to be dropped:\")\n",
    "print(duplicated_rows)\n",
    "\n",
    "concatenated_crr = concatenated_crr.drop_duplicates()\n",
    "\n",
    "concatenated_crr = concatenated_crr.loc[:, ~concatenated_crr.columns.str.contains('^Unnamed')]\n",
    "\n",
    "concatenated_crr.to_csv(fr'raw_data\\crr_full.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comply Advantage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complyadv1 = pd.read_csv(fr'{path1}\\complyadv_full.csv')\n",
    "complyadv2 = pd.read_csv(fr'{path2}\\bronze_complyadv\\{changedate}\\data_{changedate1}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_complyadv = pd.concat([complyadv1, complyadv2], ignore_index=True)\n",
    "\n",
    "duplicated_rows = concatenated_complyadv[concatenated_complyadv.duplicated()]\n",
    "\n",
    "print(\"Duplicated rows to be dropped:\")\n",
    "print(duplicated_rows)\n",
    "\n",
    "concatenated_complyadv = concatenated_complyadv.drop_duplicates()\n",
    "\n",
    "concatenated_complyadv = concatenated_complyadv.loc[:, ~concatenated_complyadv.columns.str.contains('^Unnamed')]\n",
    "\n",
    "concatenated_complyadv.to_csv(fr'raw_data\\complyadv_full.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance1 = pd.read_csv(fr'{path3}\\balance_cleaned.csv')\n",
    "balance2 = pd.read_csv(fr'{path2}\\bronze_balances\\{changedate}\\data_{changedate1}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\t\t\t\t\t\n",
    "# Extract and rename relevant columns in balances_df\n",
    "balances_df_clean = balance2[[\n",
    "    'balance_created_balance_account_id',\n",
    "    'balance_created_balance_amount',\n",
    "    'balance_created_balance_denomination',\n",
    "    'balance_created_balance_total_credit',\n",
    "    'balance_created_balance_total_debit',\n",
    "    'balance_created_balance_value_time',\n",
    "    'balance_created_balance_account_address',\n",
    "    'timestamp'\n",
    "    ]].rename(columns={\n",
    "        'balance_created_balance_account_id': 'tm_account_id', \n",
    "        'balance_created_balance_amount': 'ledger_balance',\n",
    "        'balance_created_balance_total_credit': 'total_credit',\n",
    "        'balance_created_balance_total_debit': 'total_debit',\n",
    "        'balance_created_balance_value_time': 'last_transaction_date',\n",
    "        'balance_created_balance_account_address': 'account_address',\n",
    "        'timestamp':'balance_timestamp'\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balances_df_clean = balances_df_clean.sort_values(by=['tm_account_id', 'account_address', 'last_transaction_date'])\n",
    "\n",
    "balances_df_clean_default = balances_df_clean[balances_df_clean['account_address'] == \"DEFAULT\"]\n",
    "\n",
    "balances_df_clean_default = balances_df_clean_default.drop_duplicates(subset='tm_account_id', keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balances_df_clean_default = balances_df_clean_default.loc[:, ~balances_df_clean_default.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_balance = pd.concat([balance1, balances_df_clean_default], ignore_index=True)\n",
    "\n",
    "duplicated_rows = concatenated_balance[concatenated_balance.duplicated()]\n",
    "\n",
    "print(\"Duplicated rows to be dropped:\")\n",
    "print(duplicated_rows)\n",
    "\n",
    "concatenated_balance = concatenated_balance.drop_duplicates()\n",
    "\n",
    "concatenated_balance = concatenated_balance.loc[:, ~concatenated_balance.columns.str.contains('^Unnamed')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_balance = concatenated_balance.sort_values(by=['tm_account_id', 'account_address', 'last_transaction_date'])\n",
    "\n",
    "concatenated_balance = concatenated_balance[concatenated_balance['account_address'] == \"DEFAULT\"]\n",
    "\n",
    "concatenated_balance = concatenated_balance.drop_duplicates(subset='tm_account_id', keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_balance.to_csv(fr'transformed_data\\{changedate}\\balance_cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
