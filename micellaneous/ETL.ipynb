{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.today().strftime(\"%d%m%Y\")\n",
    "path = rf'C:\\Users\\muhammadadlan\\Downloads\\customer_ultimate_beta\\raw_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_path = rf'C:\\Users\\muhammadadlan\\Downloads\\customer_ultimate_beta\\transformed_data'\n",
    "\n",
    "directory_path = rf'{transformed_path}\\{today}'\n",
    "\n",
    "os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "print(f\"Directory '{directory_path}' created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETL:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = pd.read_csv(rf'{transformed_path}\\{today}\\customer_test_v2.csv', dtype={'id':str}, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is called 'customer_data'\n",
    "columns_to_drop = [col for col in customer_df.columns if 'devices_' in col and int(col.split('_')[1]) > 0]\n",
    "\n",
    "customer_df = customer_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Sort the dataframe\n",
    "customer_df = customer_df.sort_values(['id', 'version', 'lastUpdate'], ascending=[True, True, True])\n",
    "\n",
    "# Drop duplicates based on the 'id' column, keeping the last occurrence\n",
    "customer_df = customer_df.drop_duplicates(subset=['id'], keep='last')\n",
    "\n",
    "customer_df = customer_df.loc[:, ~customer_df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "customer_df.to_csv(rf'transformed_data\\{today}\\customer_v2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique ids\n",
    "num_unique_ids = customer_df['id'].nunique()\n",
    "\n",
    "print(f'There are {num_unique_ids} unique ids in the file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Savings Acc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_df= pd.read_csv(rf'{path}\\sa_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = sa_df.shape\n",
    "print('The size of the dataset is', size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_df = sa_df.loc[:, ~sa_df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "sa_df.to_csv(rf'transformed_data\\{today}\\sa_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Savings Pot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_df= pd.read_csv(rf'{path}\\sp_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = sp_df.shape\n",
    "print('The size of the dataset is', size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_df = sp_df.loc[:, ~sp_df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "sp_df.to_csv(rf'transformed_data\\{today}\\sp_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Payments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payments_df= pd.read_csv(rf'{path}\\payments_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = payments_df.shape\n",
    "print('The size of the dataset is', size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payments_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payments_df = payments_df.loc[:, ~payments_df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "payments_df.to_csv(rf'transformed_data\\{today}\\payments_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FTV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftv_df = pd.read_csv(rf'{path}\\ftv_full.csv', dtype={'customerId':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = ftv_df.shape\n",
    "print('The size of the dataset is', size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftv_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftv_df = ftv_df.loc[:, ~ftv_df.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftv_df = ftv_df.sort_values(['tmCreatedAt'], ascending=[True])\n",
    "ftv_df = ftv_df.drop_duplicates(subset='customerId', keep='last')\n",
    "\n",
    "ftv_df.to_csv(rf'transformed_data\\{today}\\ftv_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reversal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reversal_df = pd.read_csv(rf'{path}\\reversal_full.csv', dtype={'customerId':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = reversal_df.shape\n",
    "print('The size of the dataset is', size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reversal_df = reversal_df.loc[:, ~reversal_df.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reversal_df.to_csv(rf'transformed_data\\{today}\\reversal_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Address:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_df = pd.read_csv(rf'{path}\\address_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = address_df.shape\n",
    "print('The size of the dataset is', size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the CustomerID column to be the first column\n",
    "customer_id = address_df.pop('customerId')\n",
    "address_df.insert(0, 'customerId', customer_id)\n",
    "\n",
    "# Print the modified DataFrame\n",
    "print(address_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the values in the \"subdivision\" column to uppercase\n",
    "address_df['subdivision'] = address_df['subdivision'].str.upper()\n",
    "\n",
    "# Replace long state names with abbreviations\n",
    "address_df['subdivision'] = address_df['subdivision'].str.replace(\"WILAYAH PERSEKUTUAN KUALA LUMPUR\", \"WPKL\")\n",
    "address_df['subdivision'] = address_df['subdivision'].str.replace(\"Kuala Lumpur\", \"WPKL\")\n",
    "\n",
    "address_df['subdivision'] = address_df['subdivision'].str.replace(\"WILAYAH PERSEKUTUAN PUTRAJAYA\", \"WP PUTRAJAYA\")\n",
    "address_df['subdivision'] = address_df['subdivision'].str.replace(\"WILAYAH PERSEKUTUAN LABUAN\", \"WP LABUAN\")\n",
    "\n",
    "address_df['subdivision'] = address_df['subdivision'].str.replace(\"Selangor\", \"SELANGOR\")\n",
    "address_df['subdivision'] = address_df['subdivision'].str.replace(\"SELANGOR DARUL EHSAN\", \"SELANGOR\")\n",
    "\n",
    "address_df['subdivision'] = address_df['subdivision'].str.replace(\"Melaka\", \"MELAKA\")\n",
    "\n",
    "address_df['subdivision'] = address_df['subdivision'].str.replace(\"SARAWAK\", \"SARAWAK\")\n",
    "\n",
    "address_df['subdivision'] = address_df['subdivision'].str.replace(\"Johor\", \"JOHOR\")\n",
    "address_df['subdivision'] = address_df['subdivision'].str.replace(\"JCHOR\", \"JOHOR\")\n",
    "\n",
    "address_df['subdivision'].astype('category').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform the value counts operation and assign the result to a new DataFrame\n",
    "counts_address_df = address_df['subdivision'].astype('category').value_counts().reset_index()\n",
    "\n",
    "# Rename the columns to be more descriptive\n",
    "counts_address_df.columns = ['subdivision', 'count']\n",
    "\n",
    "# Add a percentage column\n",
    "counts_address_df['percentage'] = counts_address_df['count'].apply(lambda x: round(x/counts_address_df['count'].sum() * 100, 2))\n",
    "\n",
    "# Print the modified DataFrame\n",
    "print(counts_address_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by the 'type' column in descending order\n",
    "address_df = address_df.sort_values(by='type', ascending=False)\n",
    "\n",
    "address_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_df = address_df.sort_values(by=['customerId', 'type'])\n",
    "\n",
    "df_residential = address_df[address_df['type'] == \"RESIDENTIAL\"]\n",
    "\n",
    "df_residential.to_csv(rf'transformed_data\\{today}\\residential.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_df = address_df.sort_values(by=['customerId', 'type'])\n",
    "\n",
    "df_mailing = address_df[address_df['type'] == \"MAILING\"]\n",
    "\n",
    "df_mailing.to_csv(rf'transformed_data\\{today}\\mailing.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read in the dataframes\n",
    "df_residential = pd.read_csv(rf'transformed_data\\{today}\\residential.csv')\n",
    "df_mailing = pd.read_csv(rf'transformed_data\\{today}\\mailing.csv')\n",
    "\n",
    "address_cleaned_df = pd.merge(df_residential, df_mailing, left_on='customerId', right_on='customerId', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "address_cleaned_df = address_cleaned_df.applymap(lambda x: re.sub(r'(?<!#)\\s+', ' ', str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_cleaned_df = address_cleaned_df.loc[:, ~address_cleaned_df.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_cleaned_df = address_cleaned_df.sort_values(by='lastUpdated_x', ascending=True)\n",
    "\n",
    "# Drop duplicates based on the 'id' column, keeping the last occurrence\n",
    "address_cleaned_df = address_cleaned_df.drop_duplicates(subset=['customerId'], keep='last')\n",
    "\n",
    "address_cleaned_df.to_csv(rf'transformed_data\\{today}\\address_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Employment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employment_df = pd.read_csv(rf'{path}\\employment_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_emp = employment_df.shape\n",
    "print('The size of the dataset is', size_emp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employment_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(employment_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the values in the \"employer\" column to uppercase\n",
    "employment_df['employer'] = employment_df['employer'].str.upper()\n",
    "\n",
    "# Check if the 'employer' column contains 'Rajhi' or 'Rahji'\n",
    "mask1 = employment_df['employer'].str.contains('RAJHI', na=False)\n",
    "mask2 = employment_df['employer'].str.contains('RAHJI', na=False)\n",
    "\n",
    "# Replace all occurrences of 'RAJHI' and 'RAHJI' with 'ARBM'\n",
    "employment_df.loc[mask1, 'employer'] = 'ARBM'\n",
    "employment_df.loc[mask2, 'employer'] = 'ARBM'\n",
    "\n",
    "# Check if the 'employer' column contains 'TCT'\n",
    "mask3 = employment_df['employer'].str.contains('TCT', na=False)\n",
    "\n",
    "# Replace all occurrences of 'TCT' with 'TCT TRADING SDN BHD'\n",
    "employment_df.loc[mask3, 'employer'] = 'TCT TRADING SDN BHD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employment_df['employer'].astype('category').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the CustomerID column to be the first column\n",
    "customer_id = employment_df.pop('customerId')\n",
    "employment_df.insert(0, 'customerId', customer_id)\n",
    "\n",
    "# Print the modified DataFrame\n",
    "print(employment_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "employment_df = employment_df.applymap(lambda x: re.sub(r'(?<!#)\\s+', ' ', str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employment_df = employment_df.loc[:, ~employment_df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "employment_df.to_csv(rf'transformed_data\\{today}\\employment_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV\n",
    "profile_df = pd.read_csv(rf'{path}\\profile_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_profile = profile_df.shape\n",
    "print('The size of the dataset is', size_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping columns\n",
    "profile_df = profile_df.drop(['id','religion','createdOn', 'lastUpdated'], axis=1)\n",
    "profile_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column from 'bnmCcc' to 'Bumi'\n",
    "profile_df = profile_df.rename(columns={'bnmCcc': 'Bumi'})\n",
    "# Replace values in the 'Bumi' column\n",
    "profile_df['Bumi'] = profile_df['Bumi'].replace({'Individual Bumi': True, 'Individual Non-Bumi': False})\n",
    "\n",
    "print(profile_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the CustomerID column to be the first column\n",
    "customer_id = profile_df.pop('customerId')\n",
    "profile_df.insert(0, 'customerId', customer_id)\n",
    "\n",
    "# # convert the customer ID column to a string\n",
    "# df['customerId'] = df['customerId'].astype(object)\n",
    "\n",
    "# Print the modified DataFrame\n",
    "print(profile_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_df = profile_df.loc[:, ~profile_df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "profile_df.to_csv(rf'transformed_data\\{today}\\profile_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter the DataFrame to include only rows that meet the criteria\n",
    "single_malay_bumi = profile_df[(profile_df['maritalStatus'] == 'Single') & (profile_df['ethnicity'] == 'Malay') & (profile_df['Bumi'] == True)]\n",
    "\n",
    "# Count the number of rows in the filtered DataFrame\n",
    "count = len(single_malay_bumi)\n",
    "\n",
    "# Print the count\n",
    "print('There are ' + str(count) + ' Individuals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only rows that meet the criteria\n",
    "married_malay_bumi = profile_df[(profile_df['maritalStatus'] == 'Married') & (profile_df['ethnicity'] == 'Malay') & (profile_df['Bumi'] == True)]\n",
    "\n",
    "# Count the number of rows in the filtered DataFrame\n",
    "count = len(married_malay_bumi)\n",
    "\n",
    "# Print the count\n",
    "print('There are ' + str(count) + ' Couples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the DataFrame by 'maritalStatus', 'ethnicity', and 'Bumi', and count the number of rows in each group\n",
    "count_profile_df = profile_df.groupby(['maritalStatus', 'ethnicity', 'Bumi']).size().reset_index(name='count')\n",
    "\n",
    "# Print the new table\n",
    "print(count_profile_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter the count dataframe to only include rows where maritalStatus is \"single\"\n",
    "single_df = count_profile_df[count_profile_df['maritalStatus'] == 'Single']\n",
    "\n",
    "# Plot the pie chart\n",
    "plt.pie(single_df['count'], labels=single_df['ethnicity'], autopct='%1.1f%%')\n",
    "plt.title('Distribution of Ethnicities Among Single Individuals')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Group the DataFrame by 'maritalStatus', 'ethnicity', and 'Bumi', and count the number of rows in each group\n",
    "count_df = profile_df.groupby(['maritalStatus', 'ethnicity', 'Bumi']).size().reset_index(name='count')\n",
    "\n",
    "# Create a pivot table to make it easier to plot\n",
    "pivot_table = count_df.pivot_table(index=['maritalStatus', 'ethnicity'], columns='Bumi', values='count', fill_value=0)\n",
    "\n",
    "# Create the stacked bar chart\n",
    "ax = pivot_table.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
    "\n",
    "# Set the title and axis labels\n",
    "ax.set_title('Counts by Marital Status, Ethnicity, and Bumi')\n",
    "ax.set_xlabel('Marital Status, Ethnicity')\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "# Add a legend\n",
    "ax.legend(title='Bumi', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataframe by gender and count the number of occurrences\n",
    "status_counts = profile_df['ethnicity'].value_counts()\n",
    "\n",
    "# Create a pie chart with the gender counts\n",
    "plt.pie(status_counts, labels=status_counts.index, autopct='%1.1f%%')\n",
    "plt.title('Status Distribution')\n",
    "plt.legend(title='Status Count', loc='upper right', bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crr = pd.read_csv(rf'{path}\\crr_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "crr = crr.rename(columns={'customerId':'CIC','score': 'crr_score', 'rating':'crr_rating', 'createdAt':'crr_timestamp'})\n",
    "\n",
    "# Define the columns you want to select\n",
    "column_crr = ['CIC', 'crr_score', 'crr_rating', 'crr_timestamp']\n",
    "\n",
    "# Select these columns from the DataFrame\n",
    "crr = crr[column_crr]\n",
    "\n",
    "# Convert 'timestamp' to datetime if it's not already\n",
    "crr['crr_timestamp'] = pd.to_datetime(crr['crr_timestamp'])\n",
    "\n",
    "# Sort by 'timestamp' in descending order so the latest timestamps are first\n",
    "crr = crr.sort_values('crr_timestamp', ascending=False)\n",
    "\n",
    "# Drop duplicates, keeping the first (latest) entry for each 'CIC'\n",
    "crr = crr.drop_duplicates(subset='CIC', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crr = crr.loc[:, ~crr.columns.str.contains('^Unnamed')]\n",
    "\n",
    "crr.to_csv(rf'transformed_data\\{today}\\crr_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ComplyAdv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complyadv = pd.read_csv(rf'{path}\\complyadv_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complyadv.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "complyadv = complyadv.rename(columns={'customerId':'CIC','createdAt':'complyAdv_timestamp'})\n",
    "\n",
    "# Define the columns you want to select\n",
    "column_complyadv = ['CIC', 'nameScreeningMatch', 'matchStatus', 'totalHits', 'totalBlacklistHits','complyAdv_timestamp']\n",
    "\n",
    "# Select these columns from the DataFrame\n",
    "complyadv = complyadv[column_complyadv]\n",
    "\n",
    "# Convert 'timestamp' to datetime if it's not already\n",
    "complyadv['complyAdv_timestamp'] = pd.to_datetime(complyadv['complyAdv_timestamp'])\n",
    "\n",
    "# Sort by 'timestamp' in descending order so the latest timestamps are first\n",
    "complyadv = complyadv.sort_values('complyAdv_timestamp', ascending=False)\n",
    "\n",
    "# Drop duplicates, keeping the first (latest) entry for each 'CIC'\n",
    "complyadv = complyadv.drop_duplicates(subset='CIC', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complyadv = complyadv.loc[:, ~complyadv.columns.str.contains('^Unnamed')]\n",
    "\n",
    "complyadv.to_csv(rf'transformed_data\\{today}\\complyadv_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_df = pd.read_csv(rf'{transformed_path}\\{today}\\balance_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\t\t\t\t\t\n",
    "# # Extract and rename relevant columns in balances_df\n",
    "# balance_df = balance_df[[\n",
    "#     'balance_created_balance_account_id',\n",
    "#     'balance_created_balance_amount',\n",
    "#     'balance_created_balance_denomination',\n",
    "#     'balance_created_balance_total_credit',\n",
    "#     'balance_created_balance_total_debit',\n",
    "#     'balance_created_balance_value_time',\n",
    "#     'balance_created_balance_account_address',\n",
    "#     'timestamp'\n",
    "#     ]].rename(columns={\n",
    "#         'balance_created_balance_account_id': 'tm_account_id', \n",
    "#         'balance_created_balance_amount': 'ledger_balance',\n",
    "#         'balance_created_balance_total_credit': 'total_credit',\n",
    "#         'balance_created_balance_total_debit': 'total_debit',\n",
    "#         'balance_created_balance_value_time': 'last_transaction_date',\n",
    "#         'balance_created_balance_account_address': 'account_address',\n",
    "#         'timestamp':'balance_timestamp'\n",
    "#         })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_df = balance_df.sort_values(by=['tm_account_id', 'account_address', 'last_transaction_date'])\n",
    "\n",
    "balance_df_default = balance_df[balance_df['account_address'] == \"DEFAULT\"]\n",
    "\n",
    "balance_df_default = balance_df_default.drop_duplicates(subset='tm_account_id', keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_df_default.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_df_default = balance_df_default.loc[:, ~balance_df_default.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_df_default.to_csv(rf'transformed_data\\{today}\\balance_cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
