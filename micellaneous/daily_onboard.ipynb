{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.today().strftime(\"%d%m%Y\")\n",
    "path = rf'C:\\Users\\muhammadadlan\\Downloads\\customer_ultimate_beta\\transformed_data\\{today}'\n",
    "daily = rf'C:\\Users\\muhammadadlan\\Downloads\\customer_ultimate_beta\\daily_report'\n",
    "report = rf'C:\\Users\\muhammadadlan\\Downloads\\customer_ultimate_beta\\reports'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_report = rf'{daily}\\{today}'\n",
    "\n",
    "os.makedirs(daily_report, exist_ok=True)\n",
    "\n",
    "print(f\"Directory '{daily_report}' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = rf'{report}\\{today}'\n",
    "\n",
    "os.makedirs(reports, exist_ok=True)\n",
    "\n",
    "print(f\"Directory '{reports}' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT CSVS\n",
    "customer_df = pd.read_csv(rf'{path}\\customer_v2.csv')\n",
    "sa_df = pd.read_csv(rf'{path}\\sa_v2.csv')\n",
    "sp_df = pd.read_csv(rf'{path}\\sp_v2.csv')\n",
    "payments_df = pd.read_csv(rf'{path}\\payments_v2.csv')\n",
    "ftv_df = pd.read_csv(rf'{path}\\ftv_v2.csv')\n",
    "reversal_df = pd.read_csv(rf'{path}\\reversal_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV data\n",
    "csv_data = rf'{path}\\customer_v2.csv'\n",
    "df = pd.read_csv(csv_data, dtype={'nric':str, 'id':str})\n",
    "\n",
    "# Group by NRIC and count the unique CICs\n",
    "result = df.groupby('nric')['id'].nunique().reset_index()\n",
    "\n",
    "# Rename columns for better understanding\n",
    "result.columns = ['NRIC', 'Number of unique CICs']\n",
    "\n",
    "# Display the result\n",
    "print(result)\n",
    "\n",
    "# Export the result to a CSV file\n",
    "output_file = rf'{reports}\\unique_cic.csv'\n",
    "result.to_csv(output_file, index=False)\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique ids\n",
    "num_unique_ids = customer_df['id'].nunique()\n",
    "\n",
    "print(f'There are {num_unique_ids} unique ids in the file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_cic = customer_df[(customer_df['lastname'] == 'GEOK HUI ANNA TAN')]\n",
    "\n",
    "print(check_cic['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "merged_data = (\n",
    "    customer_df.merge(ftv_df, left_on='id', right_on='customerId', how='outer')\n",
    "    .merge(reversal_df, left_on='id', right_on='customerId', how='outer')\n",
    "    .merge(payments_df, left_on='transactionId_x', right_on='transactionId', how='outer')\n",
    "    .merge(sa_df, left_on='id', right_on='customerId', suffixes=('', '_savings_account'), how='outer')\n",
    "    .merge(sp_df, left_on='id', right_on='customerId', suffixes=('', '_savings_pot'), how='outer')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.to_csv(rf'{reports}\\merged_check.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_cic = merged_data[(merged_data['lastname'] == 'MUHAMMAD ADLAN HANIF BIN ELIAS')]\n",
    "\n",
    "print(check_cic['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique ids\n",
    "num_unique_ids = merged_data['id'].nunique()\n",
    "\n",
    "print(f'There are {num_unique_ids} unique ids in the file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming reports is already defined somewhere in your code\n",
    "merge = pd.read_csv(rf'{reports}\\merged_check.csv')\n",
    "\n",
    "# Sort the merged DataFrame\n",
    "merge = merge.sort_values(by=['nric', 'lastname', 'version', 'lastUpdate', 'id'], \n",
    "                          ascending=[True, True, True, True, True])\n",
    "\n",
    "# Drop duplicates based on the 'nric' and 'lastname' columns, keeping the most recent row\n",
    "merge = merge.drop_duplicates(subset=['nric', 'lastname'], keep='last')\n",
    "\n",
    "merge.to_csv(rf'{reports}\\merged_check_v2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_cic = merge[(merge['lastname'] == 'MUHAMMAD ADLAN HANIF BIN ELIAS')]\n",
    "\n",
    "print(check_cic['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_unnamed = pd.read_csv(rf'{reports}\\merged_check_v2.csv', dtype={'nric':str, 'id':str, 'accounts_0_accountNumber':str})\n",
    "\n",
    "# remove unnamed:0 columns\n",
    "remove_unnamed = remove_unnamed.loc[:, ~remove_unnamed.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "remove_unnamed.to_csv(rf'{reports}\\merged_data_updated_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique ids\n",
    "num_unique_ids = remove_unnamed['id'].nunique()\n",
    "\n",
    "print(f'There are {num_unique_ids} unique ids in the file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "ops = pd.read_csv(rf'{reports}\\merged_data_updated_v2.csv', dtype={'nric': str, 'id': str, 'accounts_0_accountNumber': str})\n",
    "\n",
    "# Convert the columns to datetime format and convert the timestamps from UTC to Kuala Lumpur Time (GMT+8)\n",
    "# Add the names of the columns that should be datetime to this list\n",
    "datetime_columns = ['createdOn', 'lastUpdate', 'openingTimestamp', 'offboardingStartDate', 'offboardingEndDate', 'tmCreatedAt_x', 'createdAt_x', 'updatedAt_x', 'tmCreatedAt_y', 'createdAt_y', 'updatedAt_y', 'activationTimestamp', 'accounts_0_createdOn', 'accounts_0_lastUpdated', 'preferences_createdOn', 'preferences_lastUpdated', 'devices_0_lastUpdate', 'devices_0_createdOn']\n",
    "\n",
    "# Convert each datetime column to the proper format and timezone\n",
    "for column in datetime_columns:\n",
    "    ops[column] = pd.to_datetime(ops[column], errors='coerce').dt.tz_convert('Asia/Kuala_Lumpur')\n",
    "\n",
    "# The conditions for an approved customer\n",
    "is_approved = ((ops['verificationStatus'] == 'APPROVED') & (ops['openingTimestamp'].notnull()) & (ops['paymentStatus_x'] == 'COMPLETED'))\n",
    "\n",
    "# Customers who have a reversal\n",
    "has_reversal = ops['tmCreatedAt_y'].notnull()\n",
    "\n",
    "# Approved customers, regardless of whether a reversal exists or not\n",
    "approved = ops[is_approved]\n",
    "\n",
    "# Approved customers who have a reversal\n",
    "approved_with_reversal = ops[is_approved & has_reversal]\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "approved_combined = pd.concat([approved, approved_with_reversal])\n",
    "\n",
    "# Drop duplicates based on a unique identifier (I'm using 'id' here, but you should replace it with the appropriate unique identifier for your dataset)\n",
    "approved_combined = approved_combined.drop_duplicates(subset='id')\n",
    "\n",
    "# Now, approved_combined contains all unique approved customers, whether they have a reversal or not.\n",
    "\n",
    "# Drop duplicates based on lastname, keeping the row with the latest openingTimestamp\n",
    "approved_combined = approved_combined.sort_values('openingTimestamp', ascending=False).drop_duplicates(subset=['id', 'nric'], keep='first')\n",
    "\n",
    "# Filter out pending or rejected customers\n",
    "pending_or_rejected = ops[(ops['verificationStatus'] != 'APPROVED') & (~ops['id'].isin(approved_combined['id']))]\n",
    "\n",
    "# Sort by overallStatus, and openingTimestamp, and drop duplicates based on lastname, keeping the row with the highest priority overallStatus\n",
    "pending_or_rejected = pending_or_rejected.sort_values(['overallStatus', 'openingTimestamp'], ascending=[False, False])\n",
    "pending_or_rejected = pending_or_rejected.drop_duplicates('id', keep='first')\n",
    "\n",
    "# Combine the two filtered DataFrames back together\n",
    "operation = pd.concat([approved_combined, pending_or_rejected])\n",
    "\n",
    "# Remove any remaining duplicates\n",
    "operation = operation.sort_values(['openingTimestamp'], ascending=[False])\n",
    "operation = operation.drop_duplicates('id', keep='first')\n",
    "\n",
    "# Group the data by date and calculate the number of daily users for onboard_date\n",
    "onboard_grouped = operation.groupby(operation['openingTimestamp'].dt.date).agg({'id': 'count'})\n",
    "onboard_grouped = onboard_grouped.rename(columns={'id': 'Onboarding'})\n",
    "\n",
    "# Group the data by date and calculate the number of daily users for activation_date\n",
    "activation_grouped = operation.groupby(operation['tmCreatedAt_x'].dt.date).agg({'accounts_0_accountId': 'count'})\n",
    "activation_grouped = activation_grouped.rename(columns={'accounts_0_accountId': 'Activation Account'})\n",
    "\n",
    "# Define the list of offboarding status values\n",
    "offboarding_statuses = [\n",
    "    'CLOSURE_AWAITING_CLEARED',\n",
    "    'CLOSURE_AWAITING_PORTFOLIO_REMOVAL',\n",
    "    'CLOSURE_AWAITING_SAVING_POT_WITHDRAWAL',\n",
    "    'CLOSURE_AWAITING_TM_REMOVAL',\n",
    "    'CLOSURE_DEPOSIT_BLOCK_PROFIT',\n",
    "    'CLOSURE_TM_REMOVAL'\n",
    "]\n",
    "\n",
    "# Filter the DataFrame based on offboarding statuses\n",
    "offboarding_data = operation[operation['status'].isin(offboarding_statuses)]\n",
    "\n",
    "# Group the data by date and calculate the number of daily users for offboarding_date\n",
    "offboarding_grouped = offboarding_data.groupby(offboarding_data['offboardingEndDate'].dt.date).agg({'id': 'count'})\n",
    "offboarding_grouped = offboarding_grouped.rename(columns={'id': 'Offboarding'})\n",
    "\n",
    "# Combine the three groups together\n",
    "result = onboard_grouped.join(activation_grouped, how='outer')\n",
    "result = result.join(offboarding_grouped, how='outer')\n",
    "\n",
    "# Fill any missing values (NaN) with 0\n",
    "result.fillna(0, inplace=True)\n",
    "\n",
    "# Create a date range with all daily dates\n",
    "start_date = result.index.min()\n",
    "end_date = result.index.max()\n",
    "all_dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "# Reindex the result to include all daily dates\n",
    "result = result.reindex(all_dates, fill_value=0)\n",
    "result['Date'] = result.index\n",
    "\n",
    "# Show only the columns of interest and remove the index\n",
    "result = result[['Date', 'Onboarding', 'Activation Account', 'Offboarding']]\n",
    "result.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the outputs to CSV files\n",
    "today = pd.to_datetime('today').strftime('%d%m%Y')\n",
    "operation.to_csv(rf'{daily_report}\\customer_modified_{today}.csv', index=False)\n",
    "result.to_csv(rf'{daily_report}\\Daily_User_Onboard.csv', index=False)\n",
    "\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_modified = pd.read_csv(rf'{daily_report}\\customer_modified_{today}.csv', dtype={'nric':str, 'id':str, 'accounts_0_accountNumber':str})\n",
    "\n",
    "cust_modif = [\n",
    "    'id', 'createdOn', 'lastUpdate', 'version', 'firstname', 'overallStatus', 'lastname',\n",
    "    'nickname', 'phoneNumber', 'email', 'nric', 'dateOfBirth','placeOfBirth', 'mailingAddressSameAsResidence',\n",
    "    'gender', 'nationality', 'type', 'iam', 'preferences', 'devices_0_id', 'devices_0_os', 'devices_0_model',\n",
    "    'devices_0_installationId', 'devices_0_status', 'devices_0_lastUpdate', 'devices_0_createdOn', 'preferences_id',\n",
    "    'preferences_pushNotificationsAllowed', 'preferences_marketingEmailFlag', 'preferences_marketingPhoneCallFlag',\n",
    "    'preferences_marketingSmsFlag', 'preferences_marketingPushFlag', 'preferences_createdOn', 'preferences_lastUpdated',\n",
    "    'accounts_0_id', 'accounts_0_accountId', 'accounts_0_accountNumber', 'accounts_0_status', 'accounts_0_createdOn',\n",
    "    'accounts_0_lastUpdated', 'iam_id', 'iam_customer_id', 'iam_username', 'iam_secureword', 'iam_pingOneId',\n",
    "    'accounts_1_id', 'accounts_1_accountId', 'accounts_1_accountNumber', 'accounts_1_status', 'accounts_1_createdOn',\n",
    "    'accounts_1_lastUpdated', 'hasRetailsAccount', 'hasSmeAccount', 'offboardingStartDate', 'offboardingEndDate', 'offboardingStatus_string',\n",
    "    'transactionId_x', 'lifecycleId_x', 'customerAccountId_x', 'customerId_x', 'customerName_x', 'customerAccountType_x', 'customerAccountNumber_x',\n",
    "    'transferType_x', 'paymentType_x', 'amount_x', 'currency_x', 'paymentStatus_x', 'tmCreatedAt_x', 'createdAt_x', 'updatedAt_x',\n",
    "    'otherSideFullName_x', 'otherSideExternalId_x', 'otherSideAccountNumber_x', 'otherSideBankCode_x', 'otherSideAccountType_x',\n",
    "    'recipientReference_x', 'paymentDetails_x', 'proxyRegistrationNumber_x', 'errorCode_x', 'smeId', 'otherSideSmeId',\n",
    "    'amount_y', 'createdAt_y', 'currency_y', 'customerAccountId_y', 'customerAccountNumber_y', 'customerAccountType_y',\n",
    "    'customerId_y', 'customerName_y', 'errorCode_y', 'internalRequestId', 'lifecycleId_y', 'otherSideAccountNumber_y', \n",
    "    'otherSideAccountType_y', 'otherSideBankCode_y', 'otherSideExternalId_y', 'otherSideFullName_y', 'paymentDetails_y',\n",
    "    'paymentStatus_y', 'paymentType_y', 'proxyRegistrationNumber_y', 'recipientReference_y', 'tmCreatedAt_y', 'transactionId_y',\n",
    "    'transferType_y', 'updatedAt_y', 'customerId', 'transactionId', 'verificationStatus', 'reason', 'id_savings_account', 'customerId_savings_account',\n",
    "    'currency', 'status', 'number', 'openingTimestamp', 'activationTimestamp', 'moneythorAccountKey', 'id_savings_pot', 'customerId_savings_pot',\n",
    "    'moneythorAccountKey_savings_pot', 'moneythorGoalKey', 'currency_savings_pot', 'category', 'name', 'status_savings_pot', 'imageId', 'openingTimestamp_savings_pot',\n",
    "    'targetedSavingsGoal','targetedContribution', 'frequency', 'autoContribution', 'contributionDay'\n",
    "]\n",
    "\n",
    "cust_modif = customer_modified[cust_modif]\n",
    "\n",
    "cust_modif.to_csv(rf'{daily_report}\\customer_modified_{today}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def convert_to_int_if_possible(x):\n",
    "    try:\n",
    "        float_val = float(x)\n",
    "        # Check if the value is infinity or negative infinity\n",
    "        if float_val == float('inf') or float_val == -float('inf'):\n",
    "            return x  # or whatever default value you'd like to return for infinity\n",
    "        return str(int(float_val))\n",
    "    except ValueError:\n",
    "        return x\n",
    "\n",
    "\n",
    "customer_modified = pd.read_csv(fr'{daily_report}\\customer_modified_{today}.csv', dtype={'nric':str, 'id':str, 'accounts_0_accountNumber':str})\n",
    "\n",
    "cust_modif = [\n",
    "    'id', 'createdOn', 'lastUpdate', 'version', 'firstname', 'overallStatus', 'lastname',\n",
    "    'nickname', 'phoneNumber', 'email', 'nric', 'dateOfBirth','placeOfBirth', 'mailingAddressSameAsResidence',\n",
    "    'gender', 'nationality', 'type', 'iam', 'preferences', 'devices_0_id', 'devices_0_os', 'devices_0_model',\n",
    "    'devices_0_installationId', 'devices_0_status', 'devices_0_lastUpdate', 'devices_0_createdOn', 'preferences_id',\n",
    "    'preferences_pushNotificationsAllowed', 'preferences_marketingEmailFlag', 'preferences_marketingPhoneCallFlag',\n",
    "    'preferences_marketingSmsFlag', 'preferences_marketingPushFlag', 'preferences_createdOn', 'preferences_lastUpdated',\n",
    "    'accounts_0_id', 'accounts_0_accountId', 'accounts_0_accountNumber', 'accounts_0_status', 'accounts_0_createdOn',\n",
    "    'accounts_0_lastUpdated', 'iam_id', 'iam_customer_id', 'iam_username', 'iam_secureword', 'iam_pingOneId',\n",
    "    'accounts_1_id', 'accounts_1_accountId', 'accounts_1_accountNumber', 'accounts_1_status', 'accounts_1_createdOn',\n",
    "    'accounts_1_lastUpdated', 'hasRetailsAccount', 'hasSmeAccount', 'offboardingStartDate', 'offboardingEndDate', 'offboardingStatus_string',\n",
    "    'transactionId_x', 'lifecycleId_x', 'customerAccountId_x', 'customerId_x', 'customerName_x', 'customerAccountType_x', 'customerAccountNumber_x',\n",
    "    'transferType_x', 'paymentType_x', 'amount_x', 'currency_x', 'paymentStatus_x', 'tmCreatedAt_x', 'createdAt_x', 'updatedAt_x',\n",
    "    'otherSideFullName_x', 'otherSideExternalId_x', 'otherSideAccountNumber_x', 'otherSideBankCode_x', 'otherSideAccountType_x',\n",
    "    'recipientReference_x', 'paymentDetails_x', 'proxyRegistrationNumber_x', 'errorCode_x', 'smeId', 'otherSideSmeId',\n",
    "    'amount_y', 'createdAt_y', 'currency_y', 'customerAccountId_y', 'customerAccountNumber_y', 'customerAccountType_y',\n",
    "    'customerId_y', 'customerName_y', 'errorCode_y', 'internalRequestId', 'lifecycleId_y', 'otherSideAccountNumber_y', \n",
    "    'otherSideAccountType_y', 'otherSideBankCode_y', 'otherSideExternalId_y', 'otherSideFullName_y', 'paymentDetails_y',\n",
    "    'paymentStatus_y', 'paymentType_y', 'proxyRegistrationNumber_y', 'recipientReference_y', 'tmCreatedAt_y', 'transactionId_y',\n",
    "    'transferType_y', 'updatedAt_y', 'customerId', 'transactionId', 'verificationStatus', 'reason', 'id_savings_account', 'customerId_savings_account',\n",
    "    'currency', 'status', 'number', 'openingTimestamp', 'activationTimestamp', 'moneythorAccountKey', 'id_savings_pot', 'customerId_savings_pot',\n",
    "    'moneythorAccountKey_savings_pot', 'moneythorGoalKey', 'currency_savings_pot', 'category', 'name', 'status_savings_pot', 'imageId', 'openingTimestamp_savings_pot',\n",
    "    'targetedSavingsGoal','targetedContribution', 'frequency', 'autoContribution', 'contributionDay'\n",
    "]\n",
    "\n",
    "cust_modif = customer_modified[cust_modif]\n",
    "\n",
    "# List of columns that should be treated as text in the output\n",
    "text_columns = ['id', 'nric', 'accounts_0_accountNumber', 'phoneNumber', 'customerId_x', 'customerAccountNumber_x', 'otherSideExternalId_x',\n",
    "                'otherSideAccountNumber_x', 'customerAccountNumber_y', 'customerId_y', 'otherSideAccountNumber_y', 'customerId', 'customerId_savings_account',\n",
    "                'number']\n",
    "\n",
    "for column in text_columns:\n",
    "    # Only convert the column if it exists in the DataFrame\n",
    "    if column in cust_modif.columns:\n",
    "        # Apply the conversion function to each element in the column\n",
    "        cust_modif[column] = cust_modif[column].fillna(-1).apply(convert_to_int_if_possible)\n",
    "\n",
    "        # Replace the dummy value with an empty string after converting to string\n",
    "        cust_modif[column] = cust_modif[column].replace('-1', '')\n",
    "\n",
    "        # Wrap in =\"...\"\n",
    "        cust_modif[column] = '=\"' + cust_modif[column] + '\"'\n",
    "\n",
    "cust_modif.to_csv(fr'{daily_report}\\customer_modified_ops_{today}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(operation.shape)\n",
    "print(approved.shape)\n",
    "print(pending_or_rejected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "event = pd.read_csv(rf'{daily_report}\\customer_modified_ops_{today}.csv', dtype={'nric':str, 'id':str, 'accounts_0_accountNumber':str})\n",
    "\n",
    "# Select the desired columns\n",
    "selected_columns = [\n",
    "    'id', 'lastname', 'nickname', 'phoneNumber', 'email', 'nric', 'dateOfBirth', 'placeOfBirth',\n",
    "    'gender', 'nationality', 'overallStatus', 'accounts_0_status', 'type', 'accounts_0_accountId', 'accounts_0_accountNumber',\n",
    "    'transactionId_x', 'verificationStatus', 'reason', 'lifecycleId_x', 'transferType_x',\n",
    "    'paymentType_x', 'amount_x', 'currency_x', 'paymentStatus_x', 'tmCreatedAt_x', 'createdAt_x',\n",
    "    'updatedAt_x', 'otherSideFullName_x', 'otherSideExternalId_x', 'otherSideAccountNumber_x',\n",
    "    'otherSideBankCode_x', 'otherSideAccountType_x', 'recipientReference_x', 'paymentDetails_x',\n",
    "    'amount_y',\t'currency_y', 'paymentStatus_y', 'tmCreatedAt_y', 'createdAt_y', 'updatedAt_y',\n",
    "    'errorCode_y', 'internalRequestId', 'otherSideFullName_y', 'otherSideExternalId_y', 'otherSideAccountNumber_y',\n",
    "    'otherSideBankCode_y', 'otherSideAccountType_y', 'recipientReference_y', 'paymentDetails_y',\n",
    "    'openingTimestamp', 'activationTimestamp'\n",
    "]\n",
    "selected_event = event[selected_columns]\n",
    "\n",
    "# Rename the columns\n",
    "selected_event = selected_event.rename(columns={\n",
    "    'id': 'customerId',\n",
    "    'accounts_0_accountId': 'tm_account_id',\n",
    "    'accounts_0_accountNumber': 'account_number',\n",
    "    'accounts_0_status':'account_status',\n",
    "    'transactionId_x': 'transactionId',\n",
    "    'lifecycleId_x':'ftv_lifecycleId', \n",
    "    'transferType_x':'ftv_transferType',\n",
    "    'paymentType_x':'ftv_paymentType',\n",
    "    'amount_x': 'ftvAmount',\n",
    "    'currency_x': 'denominator',\n",
    "    'tmCreatedAt_x':'ftv_tmCreatedAt', \n",
    "    'createdAt_x':'ftv_createdAt',\n",
    "    'updatedAt_x':'ftv_updatedAt',\n",
    "    'otherSideFullName_x':'ftvOtherSideFullName', \n",
    "    'otherSideExternalId_x':'ftvOtherSideExternalId', \n",
    "    'otherSideAccountNumber_x':'ftvOtherSideAccountNumber',\n",
    "    'otherSideBankCode_x':'ftvOtherSideBankCode', \n",
    "    'otherSideAccountType_x':'ftvOtherSideAccountType', \n",
    "    'recipientReference_x':'ftv_recipientReference', \n",
    "    'paymentDetails_x':'ftv_paymentDetails',\n",
    "    'amount_y':'reversalAmount',\t\n",
    "    'currency_y':'reversalDenominator', \n",
    "    'paymentStatus_y':'reversal_paymentStatus', \n",
    "    'tmCreatedAt_y':'reversal_tmCreatedAt', \n",
    "    'createdAt_y':'reversal_createdAt', \n",
    "    'updatedAt_y':'reversal_updatedAt',\n",
    "    'errorCode_y':'reversal_errorCode', \n",
    "    'internalRequestId':'reversal_internalRequestId', \n",
    "    'otherSideFullName_y':'reversal_otherSideFullName', \n",
    "    'otherSideExternalId_y':'reversal_otherSideExternalId', \n",
    "    'otherSideAccountNumber_y':'reversal_otherSideAccountNumber',\n",
    "    'otherSideBankCode_y':'reversal_otherSideBankCode', \n",
    "    'otherSideAccountType_y':'reversal_otherSideAccountType', \n",
    "    'recipientReference_y':'reversal_recipientReference', \n",
    "    'paymentDetails_y':'reversal_paymentDetails',\n",
    "})\n",
    "\n",
    "\n",
    "# Print the selected and renamed DataFrame\n",
    "print(selected_event)\n",
    "selected_event.to_csv(rf'{reports}\\ops_campaign.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "checkers = pd.read_csv(rf'{reports}\\ops_campaign.csv', dtype={'nric':str, 'id':str, 'accounts_0_accountNumber':str})\n",
    "\n",
    "checkers.to_csv(rf'{daily_report}\\ops_campaign_no_truncation_{today}.csv', float_format='%.0f', index=False)\n",
    "# checkers.to_csv(rf'{daily_report}\\ops_campaign_no_truncation_{today}.dat', sep='\\t', float_format='%.0f', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Read the CSV data using pandas\n",
    "campaign_df = pd.read_csv(rf'{reports}\\ops_campaign.csv', parse_dates=['ftv_tmCreatedAt', 'openingTimestamp'], dtype={'customerId':str, 'account_number':str})\n",
    "\n",
    "# Filter the data according to the conditions\n",
    "filtered_data = campaign_df[(campaign_df['openingTimestamp'] >= '2023-07-21') &\n",
    "                     (campaign_df['verificationStatus'] == 'APPROVED')]\n",
    "\n",
    "# Count tmCreatedAt for each date and display the result\n",
    "result = filtered_data['ftv_tmCreatedAt'].dt.date.value_counts().reset_index()\n",
    "result.columns = ['Date', 'Activations']\n",
    "result.sort_values('Date', inplace=True)\n",
    "result.reset_index(drop=True, inplace=True)\n",
    "print(result)\n",
    "\n",
    "result.to_csv(rf'{daily_report}\\campaign_activations.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer Modified Balances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_df = pd.read_csv(rf'{path}\\balance_cleaned.csv')\n",
    "customer_modified_df = pd.read_csv(rf'{daily_report}\\customer_modified_{today}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = (\n",
    "    customer_modified_df.merge(balance_df, left_on='customerAccountId_x', right_on='tm_account_id', how='left')\n",
    ")\n",
    "\n",
    "merged.to_csv(rf'{reports}\\customer_with_balance_{today}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(rf'{reports}\\customer_with_balance_{today}.csv')\n",
    "\n",
    "selected_columns = [\n",
    "    'id', 'tm_account_id', 'accounts_0_accountNumber', 'createdOn','lastUpdate','version','firstname','overallStatus','lastname','nickname','phoneNumber','email','nric',\n",
    "    'dateOfBirth','placeOfBirth','mailingAddressSameAsResidence','gender','nationality','type','iam','preferences','devices_0_id','devices_0_os','devices_0_model',\n",
    "    'ledger_balance','balance_created_balance_denomination', 'total_credit', 'total_debit','last_transaction_date', 'account_address', 'balance_timestamp',\n",
    "    'devices_0_installationId','devices_0_status','devices_0_lastUpdate','devices_0_createdOn','preferences_id','preferences_pushNotificationsAllowed',\n",
    "    'preferences_marketingEmailFlag','preferences_marketingPhoneCallFlag','preferences_marketingSmsFlag','preferences_marketingPushFlag',\n",
    "    'preferences_createdOn','preferences_lastUpdated','accounts_0_accountId','accounts_0_status','accounts_0_createdOn',\n",
    "    'accounts_0_lastUpdated','iam_id','iam_customer_id','iam_username','iam_secureword','iam_pingOneId','hasRetailsAccount','hasSmeAccount',\n",
    "    'offboardingStartDate','offboardingEndDate','offboardingStatus_string','transactionId_x','lifecycleId_x','customerAccountId_x','customerId_x',\n",
    "    'customerName_x','customerAccountType_x','customerAccountNumber_x','transferType_x','paymentType_x','amount_x','currency_x','paymentStatus_x','tmCreatedAt_x',\n",
    "    'createdAt_x','updatedAt_x','otherSideFullName_x','otherSideExternalId_x','otherSideAccountNumber_x','otherSideBankCode_x','otherSideAccountType_x',\n",
    "    'recipientReference_x','paymentDetails_x','proxyRegistrationNumber_x','errorCode_x','smeId','otherSideSmeId','amount_y','createdAt_y','currency_y',\n",
    "    'customerAccountId_y','customerAccountNumber_y','customerAccountType_y','customerId_y','customerName_y','errorCode_y','internalRequestId','lifecycleId_y',\n",
    "    'otherSideAccountNumber_y','otherSideAccountType_y','otherSideBankCode_y','otherSideExternalId_y','otherSideFullName_y','paymentDetails_y','paymentStatus_y',\n",
    "    'paymentType_y','proxyRegistrationNumber_y','recipientReference_y','tmCreatedAt_y','transactionId_y','transferType_y','updatedAt_y','verificationStatus',\n",
    "    'reason','id_savings_account','customerId_savings_account','currency','status','number','openingTimestamp','activationTimestamp','moneythorAccountKey',\n",
    "    'id_savings_pot','customerId_savings_pot','moneythorAccountKey_savings_pot','moneythorGoalKey','currency_savings_pot','category','name',\n",
    "    'status_savings_pot','imageId','openingTimestamp_savings_pot','targetedSavingsGoal','targetedContribution','frequency','autoContribution',\n",
    "    'contributionDay'\n",
    "]\n",
    "selected_event = df[selected_columns]\n",
    "\n",
    "\n",
    "selected_event = selected_event.rename(columns={\n",
    "    'id': 'customer_id',\n",
    "    'createdOn': 'createdOn',\n",
    "    'lastUpdate': 'lastUpdate',\n",
    "    'version': 'version',\n",
    "    'firstname':'firstname',\n",
    "    'overallStatus': 'overallStatus',\n",
    "    'lastname': 'lastname',\n",
    "    'nickname': 'nickname',\n",
    "    'phoneNumber': 'phoneNumber',\n",
    "    'email': 'email',\n",
    "    'nric': 'nric',\n",
    "    'dateOfBirth': 'dateOfBirth',\n",
    "    'placeOfBirth': 'placeOfBirth',\n",
    "    'mailingAddressSameAsResidence': 'mailingAddressSameAsResidence',\n",
    "    'gender': 'gender',\n",
    "    'nationality': 'nationality',\n",
    "    'type': 'type',\n",
    "    'iam': 'iam',\n",
    "    'preferences': 'preferences',\n",
    "    'devices_0_id': 'device_id',\n",
    "    'devices_0_os': 'device_os',\n",
    "    'devices_0_model': 'device_model',\n",
    "    'devices_0_installationId': 'device_installationId',\n",
    "    'devices_0_status': 'device_status',\n",
    "    'devices_0_lastUpdate': 'device_lastUpdate',\n",
    "    'devices_0_createdOn': 'device_createdOn',\n",
    "    'preferences_id': 'preferences_id',\n",
    "    'preferences_pushNotificationsAllowed': 'preferences_pushNotificationsAllowed',\n",
    "    'preferences_marketingEmailFlag': 'preferences_marketingEmailFlag',\n",
    "    'preferences_marketingPhoneCallFlag': 'preferences_marketingPhoneCallFlag',\n",
    "    'preferences_marketingSmsFlag': 'preferences_marketingSmsFlag',\n",
    "    'preferences_marketingPushFlag': 'preferences_marketingPushFlag',\n",
    "    'preferences_createdOn': 'preferences_createdOn',\n",
    "    'preferences_lastUpdated': 'preferences_lastUpdated',\n",
    "    'accounts_0_accountId': 'account_accountId',\n",
    "    'accounts_0_accountNumber': 'account_accountNumber',\n",
    "    'accounts_0_status': 'account_status',\n",
    "    'accounts_0_createdOn': 'account_createdOn',\n",
    "    'accounts_0_lastUpdated': 'account_lastUpdated',\n",
    "    'iam_id': 'iam_id',\n",
    "    'iam_customer_id': 'iam_customer_id',\n",
    "    'iam_username': 'iam_username',\n",
    "    'iam_secureword': 'iam_secureword',\n",
    "    'iam_pingOneId': 'iam_pingOneId',\n",
    "    'hasRetailsAccount': 'hasRetailsAccount',\n",
    "    'hasSmeAccount': 'hasSmeAccount',\n",
    "    'offboardingStartDate': 'offboardingStartDate',\n",
    "    'offboardingEndDate': 'offboardingEndDate',\n",
    "    'offboardingStatus_string': 'offboardingStatus_string',\n",
    "    'transactionId_x': 'ftv_transactionId',\n",
    "    'lifecycleId_x': 'ftv_lifecycleId',\n",
    "    'customerAccountId_x': 'ftv_customerAccountId',\n",
    "    'customerId_x': 'ftv_customerId',\n",
    "    'customerName_x': 'ftv_customerName',\n",
    "    'customerAccountType_x': 'ftv_customerAccountType',\n",
    "    'customerAccountNumber_x': 'ftv_customerAccountNumber',\n",
    "    'transferType_x': 'ftv_transferType',\n",
    "    'paymentType_x': 'ftv_paymentType',\n",
    "    'amount_x': 'ftv_amount',\n",
    "    'currency_x': 'ftv_currency',\n",
    "    'paymentStatus_x': 'ftv_paymentStatus',\n",
    "    'tmCreatedAt_x': 'ftv_tmCreatedAt',\n",
    "    'createdAt_x': 'ftv_createdAt',\n",
    "    'updatedAt_x': 'ftv_updatedAt',\n",
    "    'otherSideFullName_x': 'ftv_otherSideFullName',\n",
    "    'otherSideExternalId_x': 'ftv_otherSideExternalId',\n",
    "    'otherSideAccountNumber_x': 'ftv_otherSideAccountNumber',\n",
    "    'otherSideBankCode_x': 'ftv_otherSideBankCode',\n",
    "    'otherSideAccountType_x': 'ftv_otherSideAccountType',\n",
    "    'recipientReference_x': 'ftv_recipientReference',\n",
    "    'paymentDetails_x': 'ftv_paymentDetails',\n",
    "    'proxyRegistrationNumber_x': 'ftv_proxyRegistrationNumber',\n",
    "    'errorCode_x': 'ftv_errorCode',\n",
    "    'smeId': 'ftv_smeId',\n",
    "    'otherSideSmeId': 'ftv_otherSideSmeId',\n",
    "    'amount_y': 'reversal_amount',\n",
    "    'createdAt_y': 'reversal_createdAt',\n",
    "    'currency_y': 'reversal_currency',\n",
    "    'customerAccountId_y': 'reversal_customerAccountId',\n",
    "    'customerAccountNumber_y': 'reversal_customerAccountNumber',\n",
    "    'customerAccountType_y': 'reversal_customerAccountType',\n",
    "    'customerId_y': 'reversal_customerId',\n",
    "    'customerName_y': 'reversal_customerName',\n",
    "    'errorCode_y': 'reversal_errorCode',\n",
    "    'internalRequestId_y': 'reversal_internalRequestId',\n",
    "    'lifecycleId_y': 'reversal_lifecycleId',\n",
    "    'otherSideAccountNumber_y': 'reversal_otherSideAccountNumber',\n",
    "    'otherSideAccountType_y': 'reversal_otherSideAccountType',\n",
    "    'otherSideBankCode_y': 'reversal_otherSideBankCode',\n",
    "    'otherSideExternalId_y': 'reversal_otherSideExternalId',\n",
    "    'otherSideFullName_y': 'reversal_otherSideFullName',\n",
    "    'paymentDetails_y': 'reversal_paymentDetails',\n",
    "    'paymentStatus_y': 'reversal_paymentStatus',\n",
    "    'paymentType_y': 'reversal_paymentType',\n",
    "    'proxyRegistrationNumber_y': 'reversal_proxyRegistrationNumber',\n",
    "    'recipientReference_y': 'reversal_recipientReference',\n",
    "    'tmCreatedAt_y': 'reversal_tmCreatedAt',\n",
    "    'transactionId_y': 'reversal_transactionId',\n",
    "    'transferType_y': 'reversal_transferType',\n",
    "    'updatedAt_y': 'reversal_updatedAt',\n",
    "    'verificationStatus': 'verificationStatus',\n",
    "    'reason': 'reason',\n",
    "    'id_savings_account': 'savings_account_id',\n",
    "    'customerId_savings_account': 'savings_account_cic',\n",
    "    'currency': 'currency',\n",
    "    'status': 'status',\n",
    "    'number': 'number',\n",
    "    'openingTimestamp': 'openingTimestamp',\n",
    "    'activationTimestamp': 'activationTimestamp',\n",
    "    'moneythorAccountKey': 'moneythorAccountKey',\n",
    "    'id_savings_pot': 'savings_pot_id',\n",
    "    'customerId_savings_pot': 'savings_pot_cic',\n",
    "    'moneythorAccountKey_savings_pot': 'savings_pot_moneythorAccountKey',\n",
    "    'moneythorGoalKey': 'savings_pot_moneythorGoalKey',\n",
    "    'currency_savings_pot': 'savings_pot_currency',\n",
    "    'category': 'savings_pot_category',\n",
    "    'name': 'savings_pot_name',\n",
    "    'status_savings_pot': 'savings_pot_status',\n",
    "    'imageId': 'savings_pot_imageid',\n",
    "    'openingTimestamp_savings_pot': 'savings_pot_openingTimestamp',\n",
    "    'targetedSavingsGoal': 'savings_pot_targetedSavingsGoal',\n",
    "    'targetedContribution': 'savings_pot_targetedContribution',\n",
    "    'frequency': 'savings_pot_frequency',\n",
    "    'autoContribution': 'savings_pot_autoContribution',\n",
    "    'contributionDay': 'savings_pot_contributionDay'\n",
    "})\n",
    "\n",
    "\n",
    "selected_event.to_csv(rf'{daily_report}\\customer_modified_balances_{today}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(rf'{reports}\\customer_with_balance_{today}.csv')\n",
    "\n",
    "def convert_to_int_if_possible(x):\n",
    "    try:\n",
    "        float_val = float(x)\n",
    "        # Check if the value is infinity or negative infinity\n",
    "        if float_val == float('inf') or float_val == -float('inf'):\n",
    "            return x  # or whatever default value you'd like to return for infinity\n",
    "        return str(int(float_val))\n",
    "    except ValueError:\n",
    "        return x\n",
    "\n",
    "    \n",
    "selected_columns = [\n",
    "    'id', 'tm_account_id', 'accounts_0_accountNumber', 'createdOn','lastUpdate','version','firstname','overallStatus','lastname','nickname','phoneNumber','email','nric',\n",
    "    'dateOfBirth','placeOfBirth','mailingAddressSameAsResidence','gender','nationality','type','iam','preferences','devices_0_id','devices_0_os','devices_0_model',\n",
    "    'ledger_balance','balance_created_balance_denomination', 'total_credit', 'total_debit','last_transaction_date', 'account_address', 'balance_timestamp',\n",
    "    'devices_0_installationId','devices_0_status','devices_0_lastUpdate','devices_0_createdOn','preferences_id','preferences_pushNotificationsAllowed',\n",
    "    'preferences_marketingEmailFlag','preferences_marketingPhoneCallFlag','preferences_marketingSmsFlag','preferences_marketingPushFlag',\n",
    "    'preferences_createdOn','preferences_lastUpdated','accounts_0_accountId','accounts_0_status','accounts_0_createdOn',\n",
    "    'accounts_0_lastUpdated','iam_id','iam_customer_id','iam_username','iam_secureword','iam_pingOneId','hasRetailsAccount','hasSmeAccount',\n",
    "    'offboardingStartDate','offboardingEndDate','offboardingStatus_string','transactionId_x','lifecycleId_x','customerAccountId_x','customerId_x',\n",
    "    'customerName_x','customerAccountType_x','customerAccountNumber_x','transferType_x','paymentType_x','amount_x','currency_x','paymentStatus_x','tmCreatedAt_x',\n",
    "    'createdAt_x','updatedAt_x','otherSideFullName_x','otherSideExternalId_x','otherSideAccountNumber_x','otherSideBankCode_x','otherSideAccountType_x',\n",
    "    'recipientReference_x','paymentDetails_x','proxyRegistrationNumber_x','errorCode_x','smeId','otherSideSmeId','amount_y','createdAt_y','currency_y',\n",
    "    'customerAccountId_y','customerAccountNumber_y','customerAccountType_y','customerId_y','customerName_y','errorCode_y','internalRequestId','lifecycleId_y',\n",
    "    'otherSideAccountNumber_y','otherSideAccountType_y','otherSideBankCode_y','otherSideExternalId_y','otherSideFullName_y','paymentDetails_y','paymentStatus_y',\n",
    "    'paymentType_y','proxyRegistrationNumber_y','recipientReference_y','tmCreatedAt_y','transactionId_y','transferType_y','updatedAt_y','verificationStatus',\n",
    "    'reason','id_savings_account','customerId_savings_account','currency','status','number','openingTimestamp','activationTimestamp','moneythorAccountKey',\n",
    "    'id_savings_pot','customerId_savings_pot','moneythorAccountKey_savings_pot','moneythorGoalKey','currency_savings_pot','category','name',\n",
    "    'status_savings_pot','imageId','openingTimestamp_savings_pot','targetedSavingsGoal','targetedContribution','frequency','autoContribution',\n",
    "    'contributionDay'\n",
    "]\n",
    "\n",
    "selected_event = df[selected_columns]\n",
    "\n",
    "\n",
    "selected_event = selected_event.rename(columns={\n",
    "    'id': 'customer_id',\n",
    "    'createdOn': 'createdOn',\n",
    "    'lastUpdate': 'lastUpdate',\n",
    "    'version': 'version',\n",
    "    'firstname':'firstname',\n",
    "    'overallStatus': 'overallStatus',\n",
    "    'lastname': 'lastname',\n",
    "    'nickname': 'nickname',\n",
    "    'phoneNumber': 'phoneNumber',\n",
    "    'email': 'email',\n",
    "    'nric': 'nric',\n",
    "    'dateOfBirth': 'dateOfBirth',\n",
    "    'placeOfBirth': 'placeOfBirth',\n",
    "    'mailingAddressSameAsResidence': 'mailingAddressSameAsResidence',\n",
    "    'gender': 'gender',\n",
    "    'nationality': 'nationality',\n",
    "    'type': 'type',\n",
    "    'iam': 'iam',\n",
    "    'preferences': 'preferences',\n",
    "    'devices_0_id': 'device_id',\n",
    "    'devices_0_os': 'device_os',\n",
    "    'devices_0_model': 'device_model',\n",
    "    'devices_0_installationId': 'device_installationId',\n",
    "    'devices_0_status': 'device_status',\n",
    "    'devices_0_lastUpdate': 'device_lastUpdate',\n",
    "    'devices_0_createdOn': 'device_createdOn',\n",
    "    'preferences_id': 'preferences_id',\n",
    "    'preferences_pushNotificationsAllowed': 'preferences_pushNotificationsAllowed',\n",
    "    'preferences_marketingEmailFlag': 'preferences_marketingEmailFlag',\n",
    "    'preferences_marketingPhoneCallFlag': 'preferences_marketingPhoneCallFlag',\n",
    "    'preferences_marketingSmsFlag': 'preferences_marketingSmsFlag',\n",
    "    'preferences_marketingPushFlag': 'preferences_marketingPushFlag',\n",
    "    'preferences_createdOn': 'preferences_createdOn',\n",
    "    'preferences_lastUpdated': 'preferences_lastUpdated',\n",
    "    'accounts_0_accountId': 'account_accountId',\n",
    "    'accounts_0_accountNumber': 'account_accountNumber',\n",
    "    'accounts_0_status': 'account_status',\n",
    "    'accounts_0_createdOn': 'account_createdOn',\n",
    "    'accounts_0_lastUpdated': 'account_lastUpdated',\n",
    "    'iam_id': 'iam_id',\n",
    "    'iam_customer_id': 'iam_customer_id',\n",
    "    'iam_username': 'iam_username',\n",
    "    'iam_secureword': 'iam_secureword',\n",
    "    'iam_pingOneId': 'iam_pingOneId',\n",
    "    'hasRetailsAccount': 'hasRetailsAccount',\n",
    "    'hasSmeAccount': 'hasSmeAccount',\n",
    "    'offboardingStartDate': 'offboardingStartDate',\n",
    "    'offboardingEndDate': 'offboardingEndDate',\n",
    "    'offboardingStatus_string': 'offboardingStatus_string',\n",
    "    'transactionId_x': 'ftv_transactionId',\n",
    "    'lifecycleId_x': 'ftv_lifecycleId',\n",
    "    'customerAccountId_x': 'ftv_customerAccountId',\n",
    "    'customerId_x': 'ftv_customerId',\n",
    "    'customerName_x': 'ftv_customerName',\n",
    "    'customerAccountType_x': 'ftv_customerAccountType',\n",
    "    'customerAccountNumber_x': 'ftv_customerAccountNumber',\n",
    "    'transferType_x': 'ftv_transferType',\n",
    "    'paymentType_x': 'ftv_paymentType',\n",
    "    'amount_x': 'ftv_amount',\n",
    "    'currency_x': 'ftv_currency',\n",
    "    'paymentStatus_x': 'ftv_paymentStatus',\n",
    "    'tmCreatedAt_x': 'ftv_tmCreatedAt',\n",
    "    'createdAt_x': 'ftv_createdAt',\n",
    "    'updatedAt_x': 'ftv_updatedAt',\n",
    "    'otherSideFullName_x': 'ftv_otherSideFullName',\n",
    "    'otherSideExternalId_x': 'ftv_otherSideExternalId',\n",
    "    'otherSideAccountNumber_x': 'ftv_otherSideAccountNumber',\n",
    "    'otherSideBankCode_x': 'ftv_otherSideBankCode',\n",
    "    'otherSideAccountType_x': 'ftv_otherSideAccountType',\n",
    "    'recipientReference_x': 'ftv_recipientReference',\n",
    "    'paymentDetails_x': 'ftv_paymentDetails',\n",
    "    'proxyRegistrationNumber_x': 'ftv_proxyRegistrationNumber',\n",
    "    'errorCode_x': 'ftv_errorCode',\n",
    "    'smeId': 'ftv_smeId',\n",
    "    'otherSideSmeId': 'ftv_otherSideSmeId',\n",
    "    'amount_y': 'reversal_amount',\n",
    "    'createdAt_y': 'reversal_createdAt',\n",
    "    'currency_y': 'reversal_currency',\n",
    "    'customerAccountId_y': 'reversal_customerAccountId',\n",
    "    'customerAccountNumber_y': 'reversal_customerAccountNumber',\n",
    "    'customerAccountType_y': 'reversal_customerAccountType',\n",
    "    'customerId_y': 'reversal_customerId',\n",
    "    'customerName_y': 'reversal_customerName',\n",
    "    'errorCode_y': 'reversal_errorCode',\n",
    "    'internalRequestId_y': 'reversal_internalRequestId',\n",
    "    'lifecycleId_y': 'reversal_lifecycleId',\n",
    "    'otherSideAccountNumber_y': 'reversal_otherSideAccountNumber',\n",
    "    'otherSideAccountType_y': 'reversal_otherSideAccountType',\n",
    "    'otherSideBankCode_y': 'reversal_otherSideBankCode',\n",
    "    'otherSideExternalId_y': 'reversal_otherSideExternalId',\n",
    "    'otherSideFullName_y': 'reversal_otherSideFullName',\n",
    "    'paymentDetails_y': 'reversal_paymentDetails',\n",
    "    'paymentStatus_y': 'reversal_paymentStatus',\n",
    "    'paymentType_y': 'reversal_paymentType',\n",
    "    'proxyRegistrationNumber_y': 'reversal_proxyRegistrationNumber',\n",
    "    'recipientReference_y': 'reversal_recipientReference',\n",
    "    'tmCreatedAt_y': 'reversal_tmCreatedAt',\n",
    "    'transactionId_y': 'reversal_transactionId',\n",
    "    'transferType_y': 'reversal_transferType',\n",
    "    'updatedAt_y': 'reversal_updatedAt',\n",
    "    'verificationStatus': 'verificationStatus',\n",
    "    'reason': 'reason',\n",
    "    'id_savings_account': 'savings_account_id',\n",
    "    'customerId_savings_account': 'savings_account_cic',\n",
    "    'currency': 'currency',\n",
    "    'status': 'status',\n",
    "    'number': 'number',\n",
    "    'openingTimestamp': 'openingTimestamp',\n",
    "    'activationTimestamp': 'activationTimestamp',\n",
    "    'moneythorAccountKey': 'moneythorAccountKey',\n",
    "    'id_savings_pot': 'savings_pot_id',\n",
    "    'customerId_savings_pot': 'savings_pot_cic',\n",
    "    'moneythorAccountKey_savings_pot': 'savings_pot_moneythorAccountKey',\n",
    "    'moneythorGoalKey': 'savings_pot_moneythorGoalKey',\n",
    "    'currency_savings_pot': 'savings_pot_currency',\n",
    "    'category': 'savings_pot_category',\n",
    "    'name': 'savings_pot_name',\n",
    "    'status_savings_pot': 'savings_pot_status',\n",
    "    'imageId': 'savings_pot_imageid',\n",
    "    'openingTimestamp_savings_pot': 'savings_pot_openingTimestamp',\n",
    "    'targetedSavingsGoal': 'savings_pot_targetedSavingsGoal',\n",
    "    'targetedContribution': 'savings_pot_targetedContribution',\n",
    "    'frequency': 'savings_pot_frequency',\n",
    "    'autoContribution': 'savings_pot_autoContribution',\n",
    "    'contributionDay': 'savings_pot_contributionDay'\n",
    "})\n",
    "\n",
    "# List of columns that should be treated as text in the output\n",
    "text_columns = ['customer_id', 'nric', 'account_accountNumber', 'phoneNumber', 'ftv_customerId', 'ftv_customerAccountNumber', 'ftv_otherSideExternalId',\n",
    "                'ftv_otherSideAccountNumber', 'reversal_customerAccountNumber', 'reversal_customerId', 'reversal_otherSideAccountNumber', 'savings_pot_cic', 'savings_account_cic',\n",
    "                'number']\n",
    "\n",
    "for column in text_columns:\n",
    "    # Only convert the column if it exists in the DataFrame\n",
    "    if column in selected_event.columns:\n",
    "        # Apply the conversion function to each element in the column\n",
    "        selected_event[column] = selected_event[column].fillna(-1).apply(convert_to_int_if_possible)\n",
    "\n",
    "        # Replace the dummy value with an empty string after converting to string\n",
    "        selected_event[column] = selected_event[column].replace('-1', '')\n",
    "\n",
    "        # Wrap in =\"...\"\n",
    "        selected_event[column] = '=\"' + selected_event[column] + '\"'\n",
    "        \n",
    "selected_event.to_csv(rf'{daily_report}\\customer_modified_balances_ops_{today}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(rf'{daily_report}\\customer_modified_balances_{today}.csv')\n",
    "\n",
    "# Count the number of unique ids\n",
    "num_unique_ids = df['customer_id'].nunique()\n",
    "\n",
    "print(f'There are {num_unique_ids} unique ids in the file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer Ultimate Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = pd.read_csv(rf'{daily_report}\\customer_modified_balances_{today}.csv')\n",
    "employment_df = pd.read_csv(rf'{path}\\employment_cleaned.csv')\n",
    "address_df = pd.read_csv(rf'{path}\\address_cleaned.csv')\n",
    "profile_df = pd.read_csv(rf'{path}\\profile_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the variable - and lowercase them\n",
    "customer_df = customer_df.rename(columns={'devices_0_os': 'device_os'})\n",
    "customer_df['device_os'] = customer_df['device_os'].str.lower()\n",
    "\n",
    "customer_df['device_os'].astype('str').str.contains('huawei').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df['device_os'].astype('category').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'deviceType' and extract either \"iOS\", \"Android\", \"Huawei\", or \"iPadOS\"\n",
    "customer_df['device_os'] = np.where(customer_df['device_os'].str.contains('ios', case=False), 'iOS',\n",
    "                   np.where(customer_df['device_os'].str.contains('huawei|huawei android', case=False), 'Huawei',\n",
    "                   np.where(customer_df['device_os'].str.contains('ipad', case=False), 'iPadOS', \n",
    "                   'Android')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df['device_os'].astype('category').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = customer_df.isnull().sum()\n",
    "total_values = customer_df.shape[0]  # Total number of rows in the dataframe\n",
    "\n",
    "# Calculate the percentage of missing values in each column\n",
    "missing_percentage = (missing_values / total_values) * 100\n",
    "\n",
    "# Combine missing values and missing percentage into a single dataframe\n",
    "missing_data = pd.DataFrame({'Missing Values': missing_values, 'Missing Percentage': missing_percentage})\n",
    "\n",
    "# Sort the dataframe by the number of missing values in descending order\n",
    "sorted_missing = missing_data.sort_values(by='Missing Values', ascending=False)\n",
    "\n",
    "# Print the top 10 columns with the highest number of missing values and their corresponding percentages\n",
    "print(sorted_missing.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = customer_df.rename(columns={'customer_id': 'customerId', 'lastname': 'fullname'})\n",
    "\n",
    "columns_to_drop = ['firstname', 'preferences', 'iam', 'proxyRegistrationNumber_x','errorCode_x','proxyRegistrationNumber_y',\n",
    "                    'otherSideExternalId_y','otherSideSmeId','smeId','iam_secureword','accounts_1_lastUpdated','accounts_1_accountNumber',\n",
    "                    'accounts_1_status','accounts_1_accountId','accounts_1_id','accounts_1_createdOn','offboardingStartDate',\n",
    "                    'paymentDetails_y','targetedContribution','offboardingEndDate','reason','customerName_y','otherSideAccountType_y',\n",
    "                    'internalRequestId','errorCode_y','currency_y']\n",
    "\n",
    "columns_to_drop_existing = list(set(columns_to_drop).intersection(customer_df.columns))\n",
    "customer_df = customer_df.drop(columns_to_drop_existing, axis=1)\n",
    "\n",
    "# remove columns that have \"Unnamed\" in their column names\n",
    "customer_df = customer_df.loc[:, ~customer_df.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df.to_csv(rf'{reports}\\customer_modified_balances_{today}.csv', index=False)\n",
    "\n",
    "# print the updated size of the dataframe\n",
    "print(f\"Updated size of the dataframe: {customer_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging\n",
    "merged_data = (\n",
    "    customer_df.merge(employment_df, left_on='customerId', right_on='customerId', how='outer')\n",
    "    .merge(address_df, left_on='customerId', right_on='customerId', how='outer')\n",
    "    .merge(profile_df, left_on='customerId', right_on='customerId', how='outer')\n",
    ")\n",
    "\n",
    "# New DF\n",
    "merged_data.to_csv(rf'{reports}\\merged_balances.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(rf'{reports}\\merged_balances.csv')\n",
    "df = df.sort_values(['version'], ascending=[True])\n",
    "\n",
    "# Drop duplicates based on the customerId' column, keeping the last row - most updated version\n",
    "print(f\"Number of rows before dropping duplicates: {len(df)}\")\n",
    "df = df.drop_duplicates(subset='customerId', keep='last')\n",
    "print(f\"Number of rows after dropping duplicates: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Updated size of the dataframe: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of missing values in each column and their percentage\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "percent_missing = round((missing_values / len(df)) * 100, 2)\n",
    "missing_df = pd.concat([missing_values, percent_missing], axis=1, keys=['Missing Count', 'Missing Percentage'])\n",
    "sorted_missing = missing_df.sort_values(by='Missing Percentage', ascending=False)\n",
    "print(sorted_missing.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns that have \"Unnamed\" in their column names\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# df = df.drop(['id_savings_pot', 'targetedSavingsGoal','customerId_savings_pot','currency_savings_pot','status_savings_pot', \n",
    "#               'category','frequency','autoContribution','contributionDay', 'moneythorAccountKey'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicate column\n",
    "subdivision_x_code = df[\"subdivision_x\"].copy()\n",
    "df[\"subdivision_x_code\"] = subdivision_x_code\n",
    "\n",
    "# assuming df is your DataFrame with the columns \"subdivision\", \"subdivision_code\", and \"type_y\"\n",
    "# get the index of the \"subdivision\" column\n",
    "subdivision_x_index = df.columns.get_loc(\"subdivision_x\")\n",
    "\n",
    "# get the index of the \"type_y\" column\n",
    "type_x_index = df.columns.get_loc(\"type_x\")\n",
    "\n",
    "# get the \"subdivision_code\" column as a Series\n",
    "subdivision_x_code_column = df.pop(\"subdivision_x_code\")\n",
    "\n",
    "# insert the \"subdivision_code\" column after the \"subdivision\" column and before the \"type_y\" column\n",
    "df.insert(subdivision_x_index + 1, \"subdivision_x_code\", subdivision_x_code_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicate column\n",
    "subdivision_y_code = df[\"subdivision_y\"].copy()\n",
    "df[\"subdivision_y_code\"] = subdivision_y_code\n",
    "\n",
    "# assuming df is your DataFrame with the columns \"subdivision\", \"subdivision_code\", and \"type_y\"\n",
    "# get the index of the \"subdivision\" column\n",
    "subdivision_y_index = df.columns.get_loc(\"subdivision_y\")\n",
    "\n",
    "# get the index of the \"type_y\" column\n",
    "type_y_index = df.columns.get_loc(\"type_y\")\n",
    "\n",
    "# get the \"subdivision_code\" column as a Series\n",
    "subdivision_y_code_column = df.pop(\"subdivision_y_code\")\n",
    "\n",
    "# insert the \"subdivision_code\" column after the \"subdivision\" column and before the \"type_y\" column\n",
    "df.insert(subdivision_y_index + 1, \"subdivision_y_code\", subdivision_y_code_column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['subdivision_x_code'] = df['subdivision_x_code'].str.replace(\"JOHOR\", \"01\")\n",
    "df['subdivision_x_code'] = df['subdivision_x_code'].str.replace(\"KEDAH\", \"02\")\n",
    "df['subdivision_x_code'] = df['subdivision_x_code'].str.replace(\"KELANTAN\", \"03\")\n",
    "df['subdivision_x_code'] = df['subdivision_x_code'].str.replace(\"MELAKA\",\"04\")\n",
    "df['subdivision_x_code'] = df['subdivision_x_code'].str.replace(\"NEGERI SEMBILAN\", \"05\")\n",
    "df['subdivision_x_code'] = df['subdivision_x_code'].str.replace(\"PAHANG\", \"06\")\n",
    "df['subdivision_x_code'] = df['subdivision_x_code'].str.replace(\"PULAU PINANG\", \"07\")\n",
    "df['subdivision_x_code'] = df['subdivision_x_code'].str.replace(\"PERAK\", \"08\")\n",
    "df['subdivision_x_code'] = df['subdivision_x_code'].str.replace(\"PERLIS\", \"09\")\n",
    "df['subdivision_x_code'] = df['subdivision_x_code'].str.replace(\"SABAH\", \"10\")\n",
    "df['subdivision_x_code'] = df['subdivision_x_code'].str.replace(\"SARAWAK\",\"11\")\n",
    "df['subdivision_x_code'] = df['subdivision_x_code'].str.replace(\"SELANGOR\", \"12\")\n",
    "df['subdivision_x_code'] = df['subdivision_x_code'].str.replace(\"TERENGGANU\", \"13\")\n",
    "df['subdivision_x_code'] = df['subdivision_x_code'].str.replace(\"WPKL\",\"14\")\n",
    "df['subdivision_x_code'] = df['subdivision_x_code'].str.replace(\"WP LABUAN\",\"15\")\n",
    "df['subdivision_x_code'] = df['subdivision_x_code'].str.replace(\"WP PUTRAJAYA\",\"16\")\n",
    "\n",
    "\n",
    "df['subdivision_y_code'] = df['subdivision_y_code'].str.replace(\"JOHOR\", \"01\")\n",
    "df['subdivision_y_code'] = df['subdivision_y_code'].str.replace(\"KEDAH\", \"02\")\n",
    "df['subdivision_y_code'] = df['subdivision_y_code'].str.replace(\"KELANTAN\", \"03\")\n",
    "df['subdivision_y_code'] = df['subdivision_y_code'].str.replace(\"MELAKA\",\"04\")\n",
    "df['subdivision_y_code'] = df['subdivision_y_code'].str.replace(\"NEGERI SEMBILAN\", \"05\")\n",
    "df['subdivision_y_code'] = df['subdivision_y_code'].str.replace(\"PAHANG\", \"06\")\n",
    "df['subdivision_y_code'] = df['subdivision_y_code'].str.replace(\"PULAU PINANG\", \"07\")\n",
    "df['subdivision_y_code'] = df['subdivision_y_code'].str.replace(\"PERAK\", \"08\")\n",
    "df['subdivision_y_code'] = df['subdivision_y_code'].str.replace(\"PERLIS\", \"09\")\n",
    "df['subdivision_y_code'] = df['subdivision_y_code'].str.replace(\"SABAH\", \"10\")\n",
    "df['subdivision_y_code'] = df['subdivision_y_code'].str.replace(\"SARAWAK\",\"11\")\n",
    "df['subdivision_y_code'] = df['subdivision_y_code'].str.replace(\"SELANGOR\", \"12\")\n",
    "df['subdivision_y_code'] = df['subdivision_y_code'].str.replace(\"TERENGGANU\", \"13\")\n",
    "df['subdivision_y_code'] = df['subdivision_y_code'].str.replace(\"WPKL\",\"14\")\n",
    "df['subdivision_y_code'] = df['subdivision_y_code'].str.replace(\"WP LABUAN\",\"15\")\n",
    "df['subdivision_y_code'] = df['subdivision_y_code'].str.replace(\"WP PUTRAJAYA\",\"16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_empty_fullname = df['fullname'].isna().sum()\n",
    "print(f\"Number of rows with empty 'fullname': {num_empty_fullname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where 'fullname' is empty\n",
    "df = df[df['fullname'].notna()]\n",
    "print(f\"Number of rows after removing empty 'fullname': {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dateOfBirth to datetime object\n",
    "# Creating age column\n",
    "df['dateOfBirth'] = pd.to_datetime(df['dateOfBirth'])\n",
    "\n",
    "# Calculate age based on current date\n",
    "currentdate = datetime.today()\n",
    "df['age'] = (currentdate - df['dateOfBirth']).astype('timedelta64[Y]').astype(int)\n",
    "\n",
    "# Insert the new \"age\" column after the \"dateOfBirth\" column\n",
    "df.insert(13, 'age', df.pop('age'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ftv_tmCreatedAt column to datetime object\n",
    "df['ftv_tmCreatedAt'] = pd.to_datetime(df['ftv_tmCreatedAt'], format='%Y-%m-%d %H:%M:%S.%f%z')\n",
    "# Format ftv_tmCreatedAt column as string with desired format\n",
    "df['ftv_tmCreatedAt'] = df['ftv_tmCreatedAt'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "# df = df.sort_values(\"ftv_tmCreatedAt\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Renaming columns\n",
    "# df = df.rename(columns={'amount_x': 'ftv_amount', \n",
    "#                         'amount_y': 'reversal_amount'})\n",
    "                        \n",
    "# Rename the columns\n",
    "df = df.rename(columns={\n",
    "                        'type_x_x':'customer_type',\n",
    "                        'createdOn_y_x': 'timestamp_employment',\n",
    "                        'line1_x': 'residential_address_line1',\n",
    "                        'line2_x': 'residential_address_line2',\n",
    "                        'line3_x': 'residential_address_line3',\n",
    "                        'line4_x': 'residential_address_line4',\n",
    "                        'line5_x': 'residential_address_line5',\n",
    "                        'countryCode_x': 'residential_address_country_code',\n",
    "                        'city_x': 'residential_address_city',\n",
    "                        'postalCode_x': 'residential_address_postal_code',\n",
    "                        'subdivision_x': 'residential_address_subdivision',\n",
    "                        'subdivision_x_code': 'residential_address_subdivision_code',\n",
    "                        'type_x': 'residential_address_type',\n",
    "                        'createdOn_x_y': 'residential_address_createdOn',\n",
    "                        'lastUpdated_x': 'residential_address_lastUpdated',\n",
    "                        'smeId_x': 'residential_address_smeId',\n",
    "                        'line1_y': 'mailing_address_line1',\n",
    "                        'line2_y': 'mailing_address_line2',\n",
    "                        'line3_y': 'mailing_address_line3',\n",
    "                        'line4_y': 'mailing_address_line4',\n",
    "                        'line5_y': 'mailing_address_line5',\n",
    "                        'countryCode_y': 'mailing_address_country_code',\n",
    "                        'city_y': 'mailing_address_city',\n",
    "                        'postalCode_y': 'mailing_address_postal_code',\n",
    "                        'subdivision_y': 'mailing_address_subdivision',\n",
    "                        'subdivision_y_code': 'mailing_address_subdivision_code',\n",
    "                        'type_y': 'mailing_address_type',\n",
    "                        'createdOn_y_y': 'mailing_address_createdOn',\n",
    "                        'lastUpdated_y': 'mailing_address_lastUpdated',\n",
    "                        'smeId_y': 'mailing_address_smeId',       \n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['id_x', 'id_y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(rf'{daily_report}\\Customer-Ultimate-Balance-{today}.csv', float_format='%.0f', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding FRAML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = pd.read_csv(rf'{daily_report}\\Customer-Ultimate-Balance-{today}.csv')\n",
    "crr_df = pd.read_csv(rf'{path}\\crr_cleaned.csv')\n",
    "complyadv_df = pd.read_csv(rf'{path}\\complyadv_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging\n",
    "merged_data = (\n",
    "    customer_df.merge(crr_df, left_on='customerId', right_on='CIC', how='outer')\n",
    "    .merge(complyadv_df, left_on='customerId', right_on='CIC', how='outer')\n",
    ")\n",
    "\n",
    "# New DF\n",
    "merged_data.to_csv(rf'{daily_report}\\Customer-Ultimate-Balance-V2-{today}.csv', float_format='%.0f', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_int_if_possible(x):\n",
    "    try:\n",
    "        return str(int(float(x)))\n",
    "    except ValueError:\n",
    "        return x\n",
    "    \n",
    "# List of columns that should be treated as text in the output\n",
    "text_columns = ['id', 'nric', 'accounts_0_accountNumber', 'phoneNumber', 'customerId_x', 'customerAccountNumber_x', 'otherSideExternalId_x',\n",
    "                'otherSideAccountNumber_x', 'customerAccountNumber_y', 'customerId_y', 'otherSideAccountNumber_y', 'customerId', 'customerId_savings_account',\n",
    "                'number']\n",
    "\n",
    "for column in text_columns:\n",
    "    # Only convert the column if it exists in the DataFrame\n",
    "    if column in merged_data.columns:\n",
    "        # Apply the conversion function to each element in the column\n",
    "        merged_data[column] = merged_data[column].fillna(-1).apply(convert_to_int_if_possible)\n",
    "\n",
    "        # Replace the dummy value with an empty string after converting to string\n",
    "        merged_data[column] = merged_data[column].replace('-1', '')\n",
    "\n",
    "        # Wrap in =\"...\"\n",
    "        merged_data[column] = '=\"' + merged_data[column] + '\"'\n",
    "        \n",
    "merged_data.to_csv(rf'{daily_report}\\Customer-Ultimate-Balance-V3-{today}.csv', float_format='%.0f', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onboarding_reject_list = pd.read_csv(rf'{daily_report}\\Customer-Ultimate-Balance-V2-{today}.csv')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have already defined 'today' variable somewhere\n",
    "# If not, you can define it using: \n",
    "crr_time = (pd.Timestamp.now() - pd.Timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "def convert_to_int_if_possible(x):\n",
    "    try:\n",
    "        return str(int(float(x)))\n",
    "    except ValueError:\n",
    "        return x\n",
    "\n",
    "text_columns = [\n",
    "    'customerId', 'nric', 'accounts_0_accountNumber', 'phoneNumber', 'customerId_x', 'customerAccountNumber_x', \n",
    "    'otherSideExternalId_x', 'otherSideAccountNumber_x', 'customerAccountNumber_y', 'customerId_y',\n",
    "    'otherSideAccountNumber_y', 'customerId', 'customerId_savings_account', 'number'\n",
    "]\n",
    "\n",
    "def wrap_with_equals(value):\n",
    "    if not value.startswith('=\"'):\n",
    "        return '=\"' + value + '\"'\n",
    "    return value\n",
    "\n",
    "for column in text_columns:\n",
    "    if column in onboarding_reject_list.columns:\n",
    "        onboarding_reject_list[column] = onboarding_reject_list[column].fillna(-1).apply(convert_to_int_if_possible)\n",
    "        onboarding_reject_list[column] = onboarding_reject_list[column].replace('-1', '')\n",
    "        onboarding_reject_list[column] = onboarding_reject_list[column].apply(wrap_with_equals)\n",
    "\n",
    "# Define the columns you want to select    \n",
    "select_cols = ['customerId','fullname', 'overallStatus','phoneNumber', 'email', \n",
    "               'device_status', 'device_lastUpdate','account_status', \n",
    "               'ftv_paymentStatus', 'verificationStatus','status',\n",
    "               'ftv_amount', 'ftv_tmCreatedAt','openingTimestamp', 'activationTimestamp','ftv_otherSideFullName', \n",
    "               'ftv_otherSideBankCode', 'crr_rating', 'matchStatus', 'nameScreeningMatch', \n",
    "               'crr_timestamp']\n",
    "\n",
    "# Convert the 'crr_timestamp' column to datetime\n",
    "onboarding_reject_list['crr_timestamp'] = pd.to_datetime(onboarding_reject_list['crr_timestamp'])\n",
    "\n",
    "# Filter the dataframe for the desired conditions\n",
    "filtered_df = onboarding_reject_list[\n",
    "    (onboarding_reject_list['overallStatus'].isin(['ONBOARDED', 'PROSPECT', 'FAILED'])) & \n",
    "    (onboarding_reject_list['crr_timestamp'].dt.strftime('%Y-%m-%d') == crr_time)\n",
    "]\n",
    "\n",
    "opf = filtered_df[select_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Rename the columns\n",
    "rename_map = {\n",
    "    'customerId': 'CIC NUMBER',\n",
    "    'overallStatus': 'OVERALL STATUS',\n",
    "    'fullname': 'NAME',\n",
    "    'phoneNumber': 'PHONE NUMBER',\n",
    "    'email': 'EMAIL',\n",
    "    'device_lastUpdate': 'DEVICE LAST UPDATE',\n",
    "    'crr_timestamp': 'CRR TIMESTAMP',\n",
    "    'nameScreeningMatch': 'NAME SCREENING MATCH',\n",
    "    'matchStatus': 'MATCH STATUS',\n",
    "    'verificationStatus': 'VERIFICATION STATUS'\n",
    "}\n",
    "\n",
    "opf = opf.rename(columns=rename_map)\n",
    "\n",
    "# Step 2: Reorder columns and add missing columns\n",
    "desired_columns_order = [\n",
    "    'CIC NUMBER', 'OVERALL STATUS', 'NAME', 'PHONE NUMBER', 'EMAIL', \n",
    "    'DEVICE LAST UPDATE', 'CRR TIMESTAMP', 'NAME SCREENING MATCH', \n",
    "    'MATCH STATUS', 'VERIFICATION STATUS', 'REJECT REASON', 'REMARKS', 'SMS1'\n",
    "]\n",
    "\n",
    "# For the columns that weren't initially in 'opf', they'll get NaN values\n",
    "for col in desired_columns_order:\n",
    "    if col not in opf.columns:\n",
    "        opf[col] = ''\n",
    "\n",
    "# Now, reorder the columns\n",
    "opf = opf[desired_columns_order]\n",
    "\n",
    "# Save the reordered and renamed DataFrame to a CSV\n",
    "opf.to_csv(rf'{daily_report}\\Onboarded and Failed List {today}.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
