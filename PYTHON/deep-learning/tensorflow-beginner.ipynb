{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Customer ID           object\n",
       "Gender                object\n",
       "Senior Citizen         int64\n",
       "Partner               object\n",
       "Dependents            object\n",
       "tenure                 int64\n",
       "Phone Service         object\n",
       "Multiple Lines        object\n",
       "Internet Service      object\n",
       "Online Security       object\n",
       "Online Backup         object\n",
       "Device Protection     object\n",
       "Tech Support          object\n",
       "Streaming TV          object\n",
       "Streaming Movies      object\n",
       "Contract              object\n",
       "Paperless Billing     object\n",
       "Payment Method        object\n",
       "Monthly Charges      float64\n",
       "Total Charges         object\n",
       "Churn                 object\n",
       "dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(df.drop(['Churn', 'Customer ID'], axis=1))\n",
    "y = df['Churn'].apply(lambda x: 1 if x=='Yes' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2501    0\n",
       "2212    0\n",
       "1594    0\n",
       "3388    0\n",
       "4780    0\n",
       "Name: Churn, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build and Compile Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RELU (hidden layer)\n",
    "\n",
    "Purpose:\n",
    "- ReLU is the most commonly used activation function in hidden layers of neural networks.\n",
    "- It outputs the input directly if it is positive; otherwise, it outputs zero. This introduces non-linearity, which helps the model learn complex patterns.\n",
    "- It‚Äôs computationally efficient because it doesn‚Äôt activate all neurons simultaneously (neurons with negative input are set to zero), which reduces the likelihood of overfitting and speeds up training.\n",
    "        \n",
    "Why ReLU?:\n",
    "- Simplicity: ReLU is simple and easy to implement.\n",
    "- Performance: It helps mitigate the vanishing gradient problem, which can occur with activation functions like the sigmoid or tanh. ReLU allows gradients to propagate more effectively - during backpropagation, which is crucial for training deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid (output layer)\n",
    "\n",
    "Purpose:\n",
    "- The sigmoid activation function squashes input values into a range between 0 and 1, making it suitable for binary classification problems (where you want an output representing a probability).\n",
    "- It transforms the output of the network into a probability value, which is ideal when your output layer is expected to represent the probability of a particular class (0 or 1).\n",
    "- This is why it's often used in the final layer of binary classification models.\n",
    "\n",
    "Why Sigmoid in the Output Layer?:\n",
    "- Binary Output: The sigmoid function is particularly useful in the output layer when you need a binary output (e.g., 0 or 1) for classification tasks.\n",
    "- Interpretability: The output can be interpreted as the probability of the input belonging to the positive class (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh (Hyperbolic Tangent)\n",
    "- Range: [-1, 1]\n",
    "- Purpose: Similar to Sigmoid but outputs values between -1 and 1. This can be advantageous because it centers the data, which often leads to faster convergence in training compared to Sigmoid.\n",
    "- Use Case: Often used in hidden layers where the output needs to be normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaky ReLU\n",
    "- Range: [‚àí‚àû,‚àû]\n",
    "- Purpose: A variation of ReLU that allows a small, non-zero gradient (controlled by ùõº) when the input is negative, addressing the \"dying ReLU\" problem where neurons could become inactive permanently.\n",
    "- Use Case: Often used in hidden layers when ReLU might result in dead neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELU (Exponential Linear Unit)\n",
    "- Range:[‚àíùõº,‚àû]\n",
    "- Purpose: Similar to Leaky ReLU but smoother. ELU can provide better performance because it has negative values, which push the mean of the activations closer to zero and speeds up learning.\n",
    "- Use Case: Applied in hidden layers, especially when a smooth gradient is preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "- Range: [0, 1] (but the sum of all outputs is 1)\n",
    "- Purpose: Converts logits (raw prediction scores) into probabilities. Unlike Sigmoid, which is used for binary classification, Softmax is used for multi-class classification where multiple classes exist, and one class must be chosen.\n",
    "- Use Case: Typically used in the output layer of neural networks when dealing with multi-class classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swish\n",
    "- Range:[‚àí0.278,‚àû]\n",
    "- Purpose: Combines properties of ReLU and Sigmoid. Swish tends to perform better than ReLU in some deep neural networks, particularly for very deep models.\n",
    "- Use Case: Can be used in hidden layers, especially in deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GELU (Gaussian Error Linear Unit)\n",
    "- Purpose: Provides a smoother version of ReLU. It introduces non-linearity in a probabilistic way and is used in some state-of-the-art models like BERT.\n",
    "- Use Case: Commonly used in deep learning architectures, particularly in natural language processing (NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softplus\n",
    "- Range:[0,‚àû)\n",
    "- Purpose: A smooth approximation of ReLU. Unlike ReLU, it is always differentiable, which can be advantageous during optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maxout\n",
    "- Purpose: Generalizes ReLU and Leaky ReLU. It allows the model to learn the best activation function for the task by selecting the maximum of multiple linear functions.\n",
    "- Use Case: Used in hidden layers, particularly in models that benefit from learning different activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "- ReLU, Leaky ReLU, ELU are commonly used in *hidden layers* for their simplicity and efficiency.\n",
    "- Sigmoid, Softmax are often used in *output layers* for binary and multi-class classification tasks, respectively.\n",
    "- Tanh, Swish, GELU are alternative activation functions that can **outperform** ReLU in certain scenarios.\n",
    "- Maxout, Softplus offer flexibility and smoothness in activations, suitable for specific use cases.\n",
    "\n",
    "Selecting the right activation function depends on the nature of the problem, the architecture of the network, and empirical testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu (Rectified Linear Unit) = most commonly used for hidden layers. easy and simple to implement\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=32, activation='relu', input_dim=len(X_train.columns)))\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Summary:\n",
    "# ReLU in hidden layers helps the model learn non-linear relationships in the data, making the network capable of solving more complex problems.\n",
    "# Sigmoid in the output layer is used for binary classification, giving a probabilistic interpretation of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate\n",
    "\n",
    "1. Learning Rate Too High\n",
    "Overshooting: If the learning rate is too high, the optimizer might take steps that are too large, causing the model to overshoot the minimum of the loss function. This can result in the model bouncing around the minimum or diverging altogether, failing to converge.\n",
    "Instability: High learning rates can lead to highly fluctuating loss values, making the training process unstable.\n",
    "Poor Convergence: The model might not find the optimal weights, leading to poor accuracy or performance.\n",
    "2. Learning Rate Too Low\n",
    "Slow Convergence: If the learning rate is too low, the optimizer takes tiny steps towards the minimum. This can lead to very slow convergence, meaning the model might take a long time to reach the optimal solution.\n",
    "Getting Stuck in Local Minima: A very low learning rate may cause the optimizer to get stuck in local minima or plateaus, preventing the model from finding the global minimum.\n",
    "Underfitting: A low learning rate might prevent the model from learning the underlying patterns in the data, leading to underfitting, where the model performs poorly on both the training and validation sets.\n",
    "3. Learning Rate Just Right\n",
    "Efficient Convergence: A well-chosen learning rate will allow the model to converge efficiently to a good minimum of the loss function. It balances the step size, ensuring the optimizer moves steadily toward the minimum without overshooting or taking too long.\n",
    "Good Generalization: With the right learning rate, the model is more likely to generalize well to unseen data, finding a balance between fitting the training data and performing well on the validation/test set.\n",
    "4. Learning Rate Scheduling\n",
    "Learning Rate Decay: Reducing the learning rate during training can help the model converge more smoothly. Start with a higher learning rate to make quick progress, and then reduce it to fine-tune the weights as you approach the minimum.\n",
    "Learning Rate Schedulers: Techniques like exponential decay, step decay, or adaptive learning rate methods (like those in Adam, RMSprop) automatically adjust the learning rate during training.\n",
    "5. Practical Considerations\n",
    "Grid Search/Cross-Validation: Often, finding the optimal learning rate involves experimentation. Techniques like grid search or cross-validation can help identify a good learning rate.\n",
    "Learning Rate Finder: A learning rate finder can be used to plot the loss against various learning rates, helping you select an optimal starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Define the optimizer with momentum\n",
    "optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "# Compile the model with the new optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- can either use categorical_cross entropy or binary_cross_entropy for classification task\n",
    "- use MSE, MAE, or Huber Loss for regression cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liist of optimizers that can be used\n",
    "1. SGD (stochastic gradient descent)\n",
    "2. SGD with momentum\n",
    "\n",
    "**Adaptive learning Rate Optimizers**\n",
    "1. Adagrad\n",
    "2. Adadelta\n",
    "3. Adam\n",
    "4. RMSProp\n",
    "5. AdaMax\n",
    "6. Nadam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "- The choice of optimizer depends on the nature of the problem, the architecture of the neural network, and specific training requirements.\n",
    "- SGD, Adam, and RMSprop are some of the most commonly used optimizers, but experimenting with different optimizers and their parameters can often lead to improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "y_train = y_train.astype('float32')  # or 'int64' if y_train contains integer labels\n",
    "\n",
    "X_test = X_test.astype('float32')\n",
    "y_test = y_test.astype('float32')  # or 'int64' if y_train contains integer labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fit, Predict and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "177/177 [==============================] - 1s 2ms/step - loss: nan - accuracy: 0.7324\n",
      "Epoch 2/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 3/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 4/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 5/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 6/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 7/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 8/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 9/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 10/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 11/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 12/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 13/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 14/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 15/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 16/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 17/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 18/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 19/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 20/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 21/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 22/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 23/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 24/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 25/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 26/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 27/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 28/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 29/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 30/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 31/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 32/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 33/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 34/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 35/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 36/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 37/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 38/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 39/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 40/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 41/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 42/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 43/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 44/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 45/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 46/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 47/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 48/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 49/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 50/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 51/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 52/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 53/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 54/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 55/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 56/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 57/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 58/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 59/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 60/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 61/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 62/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 63/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 64/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 65/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 66/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 67/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 68/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 69/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 70/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 71/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 72/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 73/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 74/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 75/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 76/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 77/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 78/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 79/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 80/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 81/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 82/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 83/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 84/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 85/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 86/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 87/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 88/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 89/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 90/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 91/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 92/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 93/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 94/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 95/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 96/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 97/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 98/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 99/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n",
      "Epoch 100/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.7354\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21102b2f370>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(X_test)\n",
    "y_hat = [0 if val < 0.5 else 1 for val in y_hat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26827537260468415"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD Binary cross entropy\n",
    "- epochs 200 - 0.8055\n",
    "- epochs 100 - 0.7821\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Saving and Reloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tfmodel\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('tfmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('tfmodel')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
